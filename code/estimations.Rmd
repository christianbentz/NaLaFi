---
title: "Estimations of Quantitative Features"
author: "Chris Bentz"
date: "14/10/2023"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Description
This file contains the code for estimating the feature values (TTR, unigram entropy, entropy rate, repetition rate) per character chunk (one per line) for each file in the NaLaFi/samples folder. The estimations are saved one per line in a csv file in NaLaFi/results/features.csv. 

## Load libraries
If the libraries are not installed yet, you need to install them using, for example, the command: install.packages("ggplot2"). For the Hrate package this is different, since it comes from github. The devtools library needs to be installed, and then the install_github() function is used.
```{r}
library(stringr)
library(entropy)
library(gsubfn)
#library(devtools)
#install_github("dimalik/Hrate")
library(Hrate)
```

## List files
Create list with all the files in the directory "data". 
```{r}
# give the path to folder with files
file.list <- list.files(path = "~/Github/NaLaFi/samples", 
                        recursive = T, full.names = T)
head(file.list)
length(file.list) 
```

# Estimations per file
```{r}
# set counter
counter = 0
# initialize dataframe to append results to
estimations.df <- data.frame(filename = character(0), corpus = character(0), 
                             subcorpus = character(0), num.char = numeric(0),
                             chunk.num = numeric(0), huni.chars = numeric (0), 
                             hrate.chars = numeric(0), ttr.chars = numeric(0), 
                             rm.chars = numeric(0)) 

# start time
start_time <- Sys.time()
for (file in file.list){
  try({ # if the processing fails for a certain file, there will be no output for this file,
  # but the try() function allows the loop to keep running  
  
  # basic processing
  # loading textfile
  textfile <- scan(file, what = "char", quote = "", comment.char = "", 
                   encoding = "UTF-8", sep = "\n" , skip = 0) 
  # print(head(textfile))
  # get filename
  filename <- basename(file) 
  print(filename) # for visual inspection
  # get corpus info
  corpus <- word(filename, 2, sep = "\\_")
  # get subcorpus info
  subcorpus <- word(filename, 3, sep = "\\_")
  #get character chunk size
  num.char <- word(filename, 1, sep = "\\_")
  
  # initizalized chunk count
  chunk.num = 0
  
  for (chunk in textfile){
    chunk.num = chunk.num + 1
    # split chunk into characters
    chars <- unlist(strsplit(chunk, ""))
    
    # preprocess text with bag-of-words normalization
    # note: this step was not taken in the original paper (Bentz 2023), 
    # but it should be taken in future studies. Without it, entropy rate estimation will sometimes fail
    # to yield realistic results due to issues with processing of special characters.
    # chars <- PreprocessText(chars)
    
    # unigram entropy estimation
    # calculate unigram entropy for characters
    chars.df <- as.data.frame(table(chars))
    # print(chars.df)
    huni.chars <- entropy(chars.df$Freq, method = "ML", unit = "log2")
  
    # entropy rate estimation
    # the values chosen for max.length and every.word will crucially 
    # impact processing time. In case of "max.length = NULL" the length is n units
    # here use the try() function since entropy rate estimation can fail when particular
    # special characters are in the strings
    hrate.chars <- try(get.estimate(text = chars, every.word = 1, max.length = NULL))
    if (class(hrate.chars) == "numeric") {
      hrate.chars <- hrate.chars
    } else {
      hrate.chars = "NA"
    }

    # calculate type-token ratio (ttr)
    ttr.chars <- nrow(chars.df)/sum(chars.df$Freq)
  
    # calculate repetition measure according to Sproat (2014)
    # the maximum number of adjacent repetitions "R" possible is the sum of frequency counts minus 1. 
    R <- sum(chars.df$Freq)-1 
    # calculate the number of adjacent repetitions "r"
    r = 0
    if (length(chars) > 1){
      for (i in 1:(length(chars)-1)){
        if (chars[i] == chars[i+1]){
          r = r + 1
        } else {
          r = r + 0
        }
      }
      # calculate the repetition measure
      rm.chars <- r/R
    } else {
      rm.chars <- "NA"  
    }
  
    # append results to dataframe
    local.df <- data.frame(filename, corpus, subcorpus, num.char, chunk.num,
                           huni.chars, hrate.chars, ttr.chars, rm.chars)
    estimations.df <- rbind(estimations.df, local.df)
  } 
  })
   # counter
  counter <- counter + 1
  print(counter)
}
end_time <- Sys.time()
end_time - start_time
#estimations.df
```

# Safe outputs to file
```{r}
write.csv(estimations.df, "~/Github/NaLaFi/results/features.csv", row.names = F)
```
