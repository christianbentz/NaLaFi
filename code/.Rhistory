paste("text", hidden, sep ="")
mlp.plot <- plot(classifier.mlp, rep = 'best')
mlp.plot
# safe plot
ggsave("~/Github/NaLaFi/figures/plot_mlp.pdf", mlp.plot, width = 10,
height = 4, dpi = 300, scale = 1, device = cairo_pdf)
library(neuralnet)
library(caret)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 10
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
estimations.subset <- estimations.df[c("corpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1], scale(estimations.subset[2:ncol(estimations.subset)]))
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
train <- estimations.scaled[datasample == 1, 1:ncol(estimations.scaled)]
# Generate test set
test <- estimations.scaled[datasample == 2, 1:ncol(estimations.scaled)]
set.seed(123)
# start time
start_time <- Sys.time()
classifier.mlp <- neuralnet(corpus == "writing" ~ .,
data = train,
hidden = hidden,
threshold = 0.1, # defaults to 0.01
rep = 10, # number of reps in which new initial values are used,
# (essentially the same as a for loop)
stepmax = 100000, # defaults to 100K
linear.output = FALSE,
algorithm = "rprop+", # defaults to "rprop+",
# i.e. resilient backpropagation
err.fct = 'ce',
act.fct = 'logistic',
likelihood = TRUE,
lifesign = 'minimal')
#classifier.mlp
end_time <- Sys.time()
end_time - start_time
mlp.plot <- plot(classifier.mlp, rep = 'best')
mlp.plot
?predict()
View(classifier.mlp)
head(classifier.mlp$result.matrix)
classifier.mlp$results.matrix[1,]
classifier.mlp$result.matrix[1,]
paste("text", hidden, sep ="")
paste("text", paste(hidden, collapse = ""), sep ="")
paste("~/Github/NaLaFi/results/MLP/results_MLP_",
paste(paste(num.char,
paste(hidden, collapse = ""),
sep =""), ".csv",
sep = ""),
sep = "")
paste(num.char, hidden, "text")
paste(num.char, hidden, "text", collapse = " ")
paste("~/Github/NaLaFi/results/MLP/results_MLP",
hidden, "_", num.char, sep = "", collapse = " ")
paste("~/Github/NaLaFi/results/MLP/results_MLP",
paste(hidden, collapse = " "),
"_", num.char, sep = "", collapse = " ")
paste("~/Github/NaLaFi/results/MLP/results_MLP",
paste(hidden, collapse = " "), "_",
paste(num.char, collapse = " "),
sep = "", collapse = " ")
paste("~/Github/NaLaFi/results/MLP/results_MLP",
paste(hidden, collapse = " "), "_",
paste(num.char, collapse = " "), ".csv",
sep = "", collapse = " ")
paste(hidden, collapse = " ")
paste("~/Github/NaLaFi/results/MLP/results_MLP",
paste(hidden, collapse = ""), "_",
paste(num.char, collapse = " "), ".csv",
sep = "", collapse = " ")
paste("~/Github/NaLaFi/results/MLP/results_MLP",
paste(hidden, collapse = ""), "_",
num.char, ".csv",
sep = "", collapse = " ")
log(1)
log(0)
log(0.8)
log(0.9)
log(0.1)
log(7)
?plot.nn()
library(neuralnet)
library(caret)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 100
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
estimations.subset <- estimations.df[c("corpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1], scale(estimations.subset[2:ncol(estimations.subset)]))
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
train <- estimations.scaled[datasample == 1, 1:ncol(estimations.scaled)]
# Generate test set
test <- estimations.scaled[datasample == 2, 1:ncol(estimations.scaled)]
library(neuralnet)
library(caret)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
estimations.subset <- estimations.df[c("corpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
num.char = 10
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
estimations.subset <- estimations.df[c("corpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
num.char = 1000
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
estimations.subset <- estimations.df[c("corpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1], scale(estimations.subset[2:ncol(estimations.subset)]))
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
train <- estimations.scaled[datasample == 1, 1:ncol(estimations.scaled)]
# Generate test set
test <- estimations.scaled[datasample == 2, 1:ncol(estimations.scaled)]
set.seed(123)
# start time
start_time <- Sys.time()
classifier.mlp <- neuralnet(corpus == "writing" ~ .,
data = train,
hidden = hidden,
threshold = 0.1, # defaults to 0.01
rep = 10, # number of reps in which new initial values are used,
# (essentially the same as a for loop)
stepmax = 100000, # defaults to 100K
linear.output = FALSE,
algorithm = "rprop+", # defaults to "rprop+",
# i.e. resilient backpropagation
err.fct = 'ce',
act.fct = 'logistic',
likelihood = TRUE,
lifesign = 'minimal')
#classifier.mlp
end_time <- Sys.time()
end_time - start_time
mlp.plot <- plot(classifier.mlp, rep = 'best', show.weights = F)
mlp.plot
hidden <- c(1)
set.seed(123)
# start time
start_time <- Sys.time()
classifier.mlp <- neuralnet(corpus == "writing" ~ .,
data = train,
hidden = hidden,
threshold = 0.1, # defaults to 0.01
rep = 10, # number of reps in which new initial values are used,
# (essentially the same as a for loop)
stepmax = 100000, # defaults to 100K
linear.output = FALSE,
algorithm = "rprop+", # defaults to "rprop+",
# i.e. resilient backpropagation
err.fct = 'ce',
act.fct = 'logistic',
likelihood = TRUE,
lifesign = 'minimal')
#classifier.mlp
end_time <- Sys.time()
end_time - start_time
mlp.plot <- plot(classifier.mlp, rep = 'best', show.weights = F)
mlp.plot
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 1000
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
estimations.subset <- estimations.df[c("corpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1], scale(estimations.subset[2:ncol(estimations.subset)]))
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
train <- estimations.scaled[datasample == 1, 1:ncol(estimations.scaled)]
# Generate test set
test <- estimations.scaled[datasample == 2, 1:ncol(estimations.scaled)]
hidden <- c(1)
set.seed(123)
# start time
start_time <- Sys.time()
classifier.mlp <- neuralnet(corpus == "writing" ~ .,
data = train,
hidden = hidden,
threshold = 0.1, # defaults to 0.01
rep = 10, # number of reps in which new initial values are used,
# (essentially the same as a for loop)
stepmax = 100000, # defaults to 100K
linear.output = FALSE,
algorithm = "rprop+", # defaults to "rprop+",
# i.e. resilient backpropagation
err.fct = 'ce',
act.fct = 'logistic',
likelihood = TRUE,
lifesign = 'minimal')
#classifier.mlp
end_time <- Sys.time()
end_time - start_time
mlp.plot <- plot(classifier.mlp, rep = 'best', show.weights = F)
mlp.plot
mlp.plot <- plot(classifier.mlp, rep = 'best', show.weights = T)
mlp.plot
hidden <- c(3,2)
set.seed(123)
# start time
start_time <- Sys.time()
classifier.mlp <- neuralnet(corpus == "writing" ~ .,
data = train,
hidden = hidden,
threshold = 0.1, # defaults to 0.01
rep = 10, # number of reps in which new initial values are used,
# (essentially the same as a for loop)
stepmax = 100000, # defaults to 100K
linear.output = FALSE,
algorithm = "rprop+", # defaults to "rprop+",
# i.e. resilient backpropagation
err.fct = 'ce',
act.fct = 'logistic',
likelihood = TRUE,
lifesign = 'minimal')
#classifier.mlp
end_time <- Sys.time()
end_time - start_time
mlp.plot <- plot(classifier.mlp, rep = 'best', show.weights = T)
mlp.plot
mlp.plot <- plot(classifier.mlp, rep = 'best', show.weights = F)
mlp.plot
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(class)
library(gridExtra)
library(gmodels)
library(caret)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 100
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
estimations.subset <- estimations.df[c("corpus", "subcorpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1:2], scale(estimations.subset[3:ncol(estimations.subset)]))
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
estimations.training <- estimations.scaled[datasample == 1, 3:ncol(estimations.scaled)]
# Generate test set
estimations.test <- estimations.scaled[datasample == 2, 3:ncol(estimations.scaled)]
training.labels <- estimations.scaled[datasample == 1, 1]
# Generate test labels
test.labels <- estimations.scaled[datasample == 2, 1]
knn.results <- data.frame(k = numeric(0), precision = numeric(0),
recall = numeric(0), f1 = numeric(0))
n = 10
# run a loop over different numbers of neighbors up to n
for (k in 1:n){
# knn estimation of labels
predictions.knn <- knn(train = as.data.frame(estimations.training),
test = as.data.frame(estimations.test),
cl = training.labels, k = k)
# model evaluation
# creating a dataframe from known (true) test labels
test.labels <- data.frame(test.labels)
# combining predicted and known classes
class.comparison <- data.frame(predictions.knn, test.labels)
# giving appropriate column names
names(class.comparison) <- c("predicted", "observed")
# inspecting our results table
head(class.comparison)
# get confusion matrix
cm <- confusionMatrix(class.comparison$predicted,
reference = class.comparison$observed)
# print(cm)
# get precision, recall, and f1 from the output list of confusionMatrix()
f1 <- cm[["byClass"]]["F1"]
recall <- cm[["byClass"]]["Recall"]
precision <- cm[["byClass"]]["Precision"]
# prepare data frame with results
local.results <- data.frame(k, precision, recall, f1, row.names = NULL)
local.results.rounded <- round(local.results, 2)
# print(local.results.rounded)
knn.results <- rbind(knn.results, local.results.rounded)
}
print(knn.results)
huni.hrate.100chars.plot <- ggplot(estimations.df,
aes(x = huni.chars, y = ttr.chars,
colour = corpus, shape = subcorpus)) +
scale_shape_manual(values = 1:nlevels(estimations.df$subcorpus)) +
geom_point(aes(fill = corpus), alpha = 0.3, size  = 1) +
labs(x = "Unigram entropy for 100 characters", y = "TTR for 100 characters") +
theme(legend.position = "none")
huni.hrate.100chars.plot <- ggMarginal(huni.hrate.100chars.plot,
groupFill = T, groupColour = T,
type = "density")
library(ggExtra)
library(ggpubr)
huni.hrate.100chars.plot <- ggplot(estimations.df,
aes(x = huni.chars, y = ttr.chars,
colour = corpus, shape = subcorpus)) +
scale_shape_manual(values = 1:nlevels(estimations.df$subcorpus)) +
geom_point(aes(fill = corpus), alpha = 0.3, size  = 1) +
labs(x = "Unigram entropy for 100 characters", y = "TTR for 100 characters") +
theme(legend.position = "none")
huni.hrate.100chars.plot <- ggMarginal(huni.hrate.100chars.plot,
groupFill = T, groupColour = T,
type = "density")
ttr.rm.100chars.plot <- ggplot(estimations.df,
aes(x = hrate.chars, y = rm.chars,
colour = corpus, shape = subcorpus)) +
scale_shape_manual(values = 1:nlevels(estimations.df$subcorpus)) +
geom_point(alpha = 0.3, size  = 1) +
theme(legend.position = "left") +
labs(x = "Entropy rate for 100 characters", y = "Repetition rate for 100 characters")
ttr.rm.100chars.plot <- ggMarginal(ttr.rm.100chars.plot,
groupFill = T, groupColour = T,
type = "density")
plots.combined <- ggarrange(huni.hrate.100chars.plot, ttr.rm.100chars.plot,
#labels = c("a)", "b)", "c)", "d)",
#           "e)", "f)"),
ncol = 2, widths = c(1, 1.3))
plots.combined
plots.combined <- ggarrange(huni.hrate.100chars.plot, ttr.rm.100chars.plot,
#labels = c("a)", "b)", "c)", "d)",
#           "e)", "f)"),
ncol = 2, widths = c(1, 1.3))
plots.combined
predictions.knn
head(estimations.df)
head(estimations.test)
nrow(estimations.test)
nrow(predictions.knn)
length(predictions.knn)
length(test.labels)
test.labels
huni.hrate.plot <- ggplot(estimations.df,
aes(x = huni.chars, y = ttr.chars,
colour = corpus)) +
geom_point(aes(fill = corpus), alpha = 0.3, size  = 1) +
labs(x = "Unigram entropy for 100 characters", y = "TTR for 100 characters") +
theme(legend.position = "none")
ttr.rm.plot <- ggplot(estimations.df,
aes(x = hrate.chars, y = rm.chars,
colour = corpus)) +
geom_point(alpha = 0.3, size  = 1) +
theme(legend.position = "left") +
labs(x = "Entropy rate for 100 characters", y = "Repetition rate for 100 characters")
plots.combined <- ggarrange(huni.hrate.plot, ttr.rm.plot,
ncol = 2, nrow = 1, widths = c(1, 1.3))
plots.combined
plots.combined <- ggarrange(huni.hrate.plot, ttr.rm.plot,
ncol = 2, nrow = 1, widths = c(1, 1.3))
plots.combined
plots.combined <- ggarrange(huni.hrate.plot, ttr.rm.plot,
ncol = 2, nrow = 1, widths = c(1, 1.3))
plots.combined
plot.df <- cbind(estimations.test, test.labels)
head(plot.df)
plot.df <- cbind(estimations.test, test.labels, predictions.knn)
head(plot.df)
huni.hrate.plot <- ggplot(plot.df,
aes(x = huni.chars, y = ttr.chars,
colour = corpus)) +
geom_point(aes(fill = corpus), alpha = 0.5, size  = 1) +
labs(x = "Unigram entropy for 100 characters", y = "TTR for 100 characters") +
theme(legend.position = "none")
ttr.rm.plot <- ggplot(plot.df,
aes(x = hrate.chars, y = rm.chars,
colour = corpus)) +
geom_point(alpha = 0.5, size  = 1) +
theme(legend.position = "left") +
labs(x = "Entropy rate for 100 characters", y = "Repetition rate for 100 characters")
plots.combined <- ggarrange(huni.hrate.plot, ttr.rm.plot,
ncol = 2, nrow = 1, widths = c(1, 1.3))
head(plot.df)
plot.df <- cbind(estimations.test, test.labels, predictions.knn)
colnames(plot.df) <- c("huni.chars", "hrate.chars", "ttr.chars",
"rm.chars", "observed", "predicted")
plot.df <- cbind(estimations.test, test.labels, predictions.knn)
colnames(plot.df) <- c("huni.chars", "hrate.chars", "ttr.chars",
"rm.chars", "observed", "predicted")
huni.hrate.plot <- ggplot(plot.df,
aes(x = huni.chars, y = ttr.chars,
colour = observed)) +
geom_point(aes(fill = predicted), alpha = 0.5, size  = 1) +
labs(x = "Unigram entropy for characters", y = "TTR for characters") +
theme(legend.position = "none")
ttr.rm.plot <- ggplot(plot.df,
aes(x = hrate.chars, y = rm.chars,
colour = observed)) +
geom_point(aes(fill = predicted), alpha = 0.5, size  = 1) +
theme(legend.position = "left") +
labs(x = "Entropy rate for characters", y = "Repetition rate for characters")
plots.combined <- ggarrange(huni.hrate.plot, ttr.rm.plot,
ncol = 2, nrow = 1, widths = c(1, 1.3))
plots.combined
sample?()
?sample()
plot.df <- cbind(estimations.test, test.labels, predictions.knn)
colnames(plot.df) <- c("huni.chars", "hrate.chars", "ttr.chars",
"rm.chars", "observed", "predicted")
# downsample to better see points
plot.df <- sample_n(plot.df, 100)
k
head(plot.df)
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
library(ggplot2)
library(ggridges)
library(plyr)
file.list <- list.files(path = "~/Github/NaLaFi/data/",
recursive = T, full.names = T)
#print(file.list)
length(file.list)
head(file.list)
0.5/0.5
log(1)
0.8/0.2
log(4)
0.2/0.8
log(0.25)
log(0.001)
4*0.65+2*1.69+0.5*-7.9+0.1*1.62
2.192-3.78
-1.588*10
1/(1-e^11.88)
1/(1-2.72^11.88)
1/(1-2.72^5)
1/(1-2.72^-2)
1/(1-2.72^2)
1/(1-2.72^-1)
1/(1+2.72^11.88)
1/(1+2.72^2)
1/(1+2.72^10)
1/(1+2.72^1)
1/(1+2.72^0.5)
1/(1+2.72^0.1)
1/(1+2.72^-0.1)
1/(1+2.72^0)
1/(1+2.72^-1)
1/(1+2.72^-10)
4*0.65+3*1.69+0.5*-7.9+0.1*1.62
3.882-3.78
0.102*-10.76
-1.09752+3.927
1/(1+2.72^-2.82)
knitr::opts_chunk$set(echo = TRUE)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 100
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
nrow(estimations.df)
estimations.subset <- estimations.df[c("corpus", "subcorpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1:2], scale(estimations.subset[3:ncol(estimations.subset)]))
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
estimations.training <- estimations.scaled[datasample == 1, 3:ncol(estimations.scaled)]
estimations.test <- estimations.scaled[datasample == 2, 3:ncol(estimations.scaled)]
nrow(estimations.training)
nrow(estimations.test)
2044+963
