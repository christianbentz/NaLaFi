class.comparison <- data.frame(mlp.predictions.rd, test.labels)
# giving appropriate column names
names(class.comparison) <- c("predicted", "observed")
# inspecting our results table
head(class.comparison)
# get confusion matrix
cm <- confusionMatrix(as.factor(class.comparison$predicted),
reference = as.factor(class.comparison$observed))
#print(cm)
# get precision, recall, and f1 from the output list of confusionMatrix()
accuracy <- cm$overall['Accuracy']
f1 <- cm[["byClass"]]["F1"]
recall <- cm[["byClass"]]["Recall"]
precision <- cm[["byClass"]]["Precision"]
# unname and round resulting values
accuracy <- round(unname(accuracy), 2)
f1 <- round(unname(f1), 2)
recall <- round(unname(recall), 2)
precision <- round(unname(precision), 2)
# append results to dataframe
hidden.structure <- paste(unlist(hidden), collapse = ",")
hidden.size <- sum(unlist(hidden))
hidden.depth <- length(hidden)
local.df <- data.frame(num.char, hidden.structure, hidden.size,
hidden.depth, err.fct, act.fct.name, algorithm,
accuracy, precision, recall, f1)
results.df <- rbind(results.df, local.df)
})
counter <- counter + 1
print(counter)
}
end_time <- Sys.time()
proc.time <- end_time - start_time
print(proc.time)
hidden.list <- list(c(5,3,2))
# initialize dataframe to append results to
results.df <- data.frame(num.char = numeric(0), hidden.structure = character(0),
hidden.size = numeric(0), hidden.depth = numeric(0),
err.fct = character(0), act.fct.name = character(0),
algorithm = character(0), accuracy = numeric(0),
precision = numeric(0), recall = numeric(0),
f1 = numeric(0)
)
# Custom activation function
# Note that this function only works with err.fct = 'sse'
softplus <- function(x) log(1 + exp(x))
# set random seed for reproducibility
set.seed(123)
# counter
counter <- 0
# start time
start_time <- Sys.time()
for (hidden in hidden.list) {
# Training
# choose error function
err.fct = 'sse' # options: 'sse', 'ce'
# choose activation function
act.fct = 'tanh' # options: 'logistic', 'tanh', softplus, relu,
# beware: the last two options have to be written without quotes ''
# note: softplus, relu, and tanh only work properly with sse as err.fct
act.fct.name = 'tanh' # make sure to give name as character string (for later storage)
# choose algorithm
algorithm = 'rprop+' # options: 'rprop+', 'rprop-', 'backprop', 'sag', 'slr'
# train classifier with neuralnet() function
try({ # if the processing failes for a certain file, there will be no output for this file,
# but the try() function allows the loop to keep running
classifier.mlp <- neuralnet(corpus == "writing" ~ .,
data = train,
hidden = hidden,
threshold = 0.1, # defaults to 0.01
rep = 1, # number of reps in which new initial values are used,
# (essentially the same as a for loop)
stepmax = 100000, # defaults to 100K
linear.output = FALSE,
algorithm = algorithm, # defaults to "rprop+",
# i.e. resilient backpropagation
# learningrate = 0.1,
err.fct = err.fct,
act.fct = act.fct,
likelihood = TRUE,
lifesign = 'minimal')
classifier.mlp
# results matrix (each column represents one repetition)
classifier.mlp$result.matrix
# Prediction
# Get prediction using the predict() function
mlp.predictions <- predict(classifier.mlp, test,
rep = which.min(classifier.mlp$result.matrix[1,]), # Predict response values
# based on the "best" repetition (epoche), i.e. the one with the lowest error.
all.units = FALSE)
# assign a label according to the rule that the label is "writing"
# if the prediction probability is >0.5, else assign "non-writing".
mlp.predictions.rd <- ifelse(mlp.predictions > 0.5, "writing", "non-writing")
#head(mlp.predictions.rd, 10)
#table(test$corpus == "non-writing", predictions[, 1] > 0.5)
# Model Evaluation
# creating a dataframe from known (true) test labels
test.labels <- data.frame(test$corpus)
# combining predicted and known classes
class.comparison <- data.frame(mlp.predictions.rd, test.labels)
# giving appropriate column names
names(class.comparison) <- c("predicted", "observed")
# inspecting our results table
head(class.comparison)
# get confusion matrix
cm <- confusionMatrix(as.factor(class.comparison$predicted),
reference = as.factor(class.comparison$observed))
#print(cm)
# get precision, recall, and f1 from the output list of confusionMatrix()
accuracy <- cm$overall['Accuracy']
f1 <- cm[["byClass"]]["F1"]
recall <- cm[["byClass"]]["Recall"]
precision <- cm[["byClass"]]["Precision"]
# unname and round resulting values
accuracy <- round(unname(accuracy), 2)
f1 <- round(unname(f1), 2)
recall <- round(unname(recall), 2)
precision <- round(unname(precision), 2)
# append results to dataframe
hidden.structure <- paste(unlist(hidden), collapse = ",")
hidden.size <- sum(unlist(hidden))
hidden.depth <- length(hidden)
local.df <- data.frame(num.char, hidden.structure, hidden.size,
hidden.depth, err.fct, act.fct.name, algorithm,
accuracy, precision, recall, f1)
results.df <- rbind(results.df, local.df)
})
counter <- counter + 1
print(counter)
}
end_time <- Sys.time()
proc.time <- end_time - start_time
print(proc.time)
hidden.list <- list(c(4,4))
# initialize dataframe to append results to
results.df <- data.frame(num.char = numeric(0), hidden.structure = character(0),
hidden.size = numeric(0), hidden.depth = numeric(0),
err.fct = character(0), act.fct.name = character(0),
algorithm = character(0), accuracy = numeric(0),
precision = numeric(0), recall = numeric(0),
f1 = numeric(0)
)
# Custom activation function
# Note that this function only works with err.fct = 'sse'
softplus <- function(x) log(1 + exp(x))
# set random seed for reproducibility
set.seed(123)
# counter
counter <- 0
# start time
start_time <- Sys.time()
for (hidden in hidden.list) {
# Training
# choose error function
err.fct = 'sse' # options: 'sse', 'ce'
# choose activation function
act.fct = 'tanh' # options: 'logistic', 'tanh', softplus, relu,
# beware: the last two options have to be written without quotes ''
# note: softplus, relu, and tanh only work properly with sse as err.fct
act.fct.name = 'tanh' # make sure to give name as character string (for later storage)
# choose algorithm
algorithm = 'rprop+' # options: 'rprop+', 'rprop-', 'backprop', 'sag', 'slr'
# train classifier with neuralnet() function
try({ # if the processing failes for a certain file, there will be no output for this file,
# but the try() function allows the loop to keep running
classifier.mlp <- neuralnet(corpus == "writing" ~ .,
data = train,
hidden = hidden,
threshold = 0.1, # defaults to 0.01
rep = 1, # number of reps in which new initial values are used,
# (essentially the same as a for loop)
stepmax = 100000, # defaults to 100K
linear.output = FALSE,
algorithm = algorithm, # defaults to "rprop+",
# i.e. resilient backpropagation
# learningrate = 0.1,
err.fct = err.fct,
act.fct = act.fct,
likelihood = TRUE,
lifesign = 'minimal')
classifier.mlp
# results matrix (each column represents one repetition)
classifier.mlp$result.matrix
# Prediction
# Get prediction using the predict() function
mlp.predictions <- predict(classifier.mlp, test,
rep = which.min(classifier.mlp$result.matrix[1,]), # Predict response values
# based on the "best" repetition (epoche), i.e. the one with the lowest error.
all.units = FALSE)
# assign a label according to the rule that the label is "writing"
# if the prediction probability is >0.5, else assign "non-writing".
mlp.predictions.rd <- ifelse(mlp.predictions > 0.5, "writing", "non-writing")
#head(mlp.predictions.rd, 10)
#table(test$corpus == "non-writing", predictions[, 1] > 0.5)
# Model Evaluation
# creating a dataframe from known (true) test labels
test.labels <- data.frame(test$corpus)
# combining predicted and known classes
class.comparison <- data.frame(mlp.predictions.rd, test.labels)
# giving appropriate column names
names(class.comparison) <- c("predicted", "observed")
# inspecting our results table
head(class.comparison)
# get confusion matrix
cm <- confusionMatrix(as.factor(class.comparison$predicted),
reference = as.factor(class.comparison$observed))
#print(cm)
# get precision, recall, and f1 from the output list of confusionMatrix()
accuracy <- cm$overall['Accuracy']
f1 <- cm[["byClass"]]["F1"]
recall <- cm[["byClass"]]["Recall"]
precision <- cm[["byClass"]]["Precision"]
# unname and round resulting values
accuracy <- round(unname(accuracy), 2)
f1 <- round(unname(f1), 2)
recall <- round(unname(recall), 2)
precision <- round(unname(precision), 2)
# append results to dataframe
hidden.structure <- paste(unlist(hidden), collapse = ",")
hidden.size <- sum(unlist(hidden))
hidden.depth <- length(hidden)
local.df <- data.frame(num.char, hidden.structure, hidden.size,
hidden.depth, err.fct, act.fct.name, algorithm,
accuracy, precision, recall, f1)
results.df <- rbind(results.df, local.df)
})
counter <- counter + 1
print(counter)
}
end_time <- Sys.time()
proc.time <- end_time - start_time
print(proc.time)
mlp.plot <- plot(classifier.mlp, rep = 'best', show.weights = F)
mlp.plot
70+47+33+30+11+6+4+3+3
50+3+17
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(class)
library(gridExtra)
library(gmodels)
library(caret)
library(ggExtra)
library(ggpubr)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 10
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
nrow(estimations.df)
estimations.subset <- estimations.df[c("corpus", "subcorpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1:2], scale(estimations.subset[3:ncol(estimations.subset)]))
nrow(estimations.scaled)
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
estimations.training <- estimations.scaled[datasample == 1, 3:ncol(estimations.scaled)]
nrow(estimations.training)
# Generate test set
estimations.test <- estimations.scaled[datasample == 2, 3:ncol(estimations.scaled)]
nrow(estimations.test)
2543+1198
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 100
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
nrow(estimations.df)
estimations.subset <- estimations.df[c("corpus", "subcorpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1:2], scale(estimations.subset[3:ncol(estimations.subset)]))
nrow(estimations.scaled)
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
estimations.training <- estimations.scaled[datasample == 1, 3:ncol(estimations.scaled)]
nrow(estimations.training)
# Generate test set
estimations.test <- estimations.scaled[datasample == 2, 3:ncol(estimations.scaled)]
nrow(estimations.test)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 1000
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
nrow(estimations.df)
estimations.subset <- estimations.df[c("corpus", "subcorpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1:2], scale(estimations.subset[3:ncol(estimations.subset)]))
nrow(estimations.scaled)
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
estimations.training <- estimations.scaled[datasample == 1, 3:ncol(estimations.scaled)]
nrow(estimations.training)
# Generate test set
estimations.test <- estimations.scaled[datasample == 2, 3:ncol(estimations.scaled)]
nrow(estimations.test)
knitr::opts_chunk$set(echo = TRUE)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
estimations10.df <- estimations.df[estimations.df$num.char == 10, ]
estimations100.df <- estimations.df[estimations.df$num.char == 100, ]
estimations1000.df <- estimations.df[estimations.df$num.char == 1000,]
huni.hrate.100chars.plot <- ggplot(estimations100.df,
aes(x = huni.chars, y = ttr.chars,
colour = corpus, shape = subcorpus)) +
scale_shape_manual(values = 1:length(unique(estimations100.df$subcorpus))) +
geom_point(aes(fill = corpus), alpha = 0.3, size  = 1) +
labs(x = "Unigram entropy for 100 characters", y = "TTR for 100 characters") +
theme(legend.position = "none")
huni.hrate.100chars.plot <- ggMarginal(huni.hrate.100chars.plot,
groupFill = T, groupColour = T,
type = "density")
huni.hrate.100chars.plot
huni.hrate.100chars.plot <- ggplot(estimations100.df,
aes(x = huni.chars, y = ttr.chars,
colour = corpus, shape = subcorpus)) +
scale_shape_manual(values = 1:length(unique(estimations100.df$subcorpus))) +
geom_point(aes(fill = corpus), alpha = 0.3, size  = 1) +
labs(x = "Unigram entropy for 100 characters", y = "TTR for 100 characters") +
theme(legend.position = "none")
huni.hrate.100chars.plot <- ggMarginal(huni.hrate.100chars.plot,
groupFill = T, groupColour = T,
type = "density")
huni.hrate.100chars.plot
library(ggplot2)
library(dplyr)
library(class)
library(gridExtra)
library(gmodels)
library(caret)
library(ggExtra)
library(ggpubr)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
selected <- c("heraldics","procunei","animal","random","morse","shuffled","weather","natural")
estimations.df <- estimations.df[!(estimations.df$subcorpus %in% selected), ]
num.char = 100
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
nrow(estimations.df)
View(estimations.df)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
selected <- c("heraldics",
"procunei",
"animal",
"random",
"morse",
"shuffled",
#"weather",
"natural",
"pycode")
estimations.df <- estimations.df[!(estimations.df$subcorpus %in% selected), ]
View(estimations.df)
# load estimations from stringBase corpus
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
selected <- c(#"heraldics",
"procunei",
"animal",
"random",
"morse",
"shuffled",
"weather",
"natural",
"pycode")
estimations.df <- estimations.df[!(estimations.df$subcorpus %in% selected), ]
View(estimations.df)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
selected <- c(#"heraldics",
"procunei",
"animal",
"random",
"morse",
"shuffled",
"weather",
"natural",
"pycode")
estimations.df <- estimations.df[!(estimations.df$subcorpus %in% selected), ]
View(estimations.df)
571*0.98
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(class)
library(gmodels)
library(caret)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 1000
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
estimations.subset <- estimations.df[c("corpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1], scale(estimations.subset[2:ncol(estimations.subset)]))
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
train <- estimations.scaled[datasample == 1, 1:ncol(estimations.scaled)]
# Generate test set
test <- estimations.scaled[datasample == 2, 1:ncol(estimations.scaled)]
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(class)
library(gridExtra)
library(gmodels)
library(caret)
library(ggExtra)
library(ggpubr)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 100
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
nrow(estimations.df)
estimations.subset <- estimations.df[c("corpus", "subcorpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1:2], scale(estimations.subset[3:ncol(estimations.subset)]))
nrow(estimations.scaled)
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
estimations.training <- estimations.scaled[datasample == 1, 3:ncol(estimations.scaled)]
nrow(estimations.training)
# Generate test set
estimations.test <- estimations.scaled[datasample == 2, 3:ncol(estimations.scaled)]
nrow(estimations.test)
training.labels <- estimations.scaled[datasample == 1, 1]
# Generate test labels
test.labels <- estimations.scaled[datasample == 2, 1]
knn.results <- data.frame(k = numeric(0), accuracy = numeric(0), precision = numeric(0),
recall = numeric(0), f1 = numeric(0))
n = 10
k=5
predictions.knn <- knn(train = as.data.frame(estimations.training),
test = as.data.frame(estimations.test),
cl = training.labels, k = k)
predictions.knn
test.labels <- data.frame(test.labels)
class.comparison <- data.frame(predictions.knn, test.labels)
# giving appropriate column names
names(class.comparison) <- c("predicted", "observed")
class.comparison
head(class.comparison)
head(class.comparison)
head(training.labels)
as.data.frame(estimations.training)
head(estimations.training)
head(estimations.test)
head(datasample)
head(estimations.scaled)
head(estimations.df)
nrow(estimations.subset)
head(estimations.training)
head(estimations.test)
head(estimations.scaled)
head(estimations.df)
write.csv(estimations.df, file ="")
file.choose()
write.csv(estimations.df, file ="/home/chris/Desktop/estimationsdf.jpg")
write.csv(estimations.df, file ="/home/chris/Desktop/estimationsdf.csv")
head(class.comparison)
head(predictions.knn)
head(estimations.test)
write.csv(estimations.test)
write.csv(estimations.test, file ="/home/chris/Desktop/estimationstest.csv")
View(class.comparison)
head(estimations.test)
head(estimations.df)
merge(estimations.df, estimations.test)
merge(estimations.df, estimations.test, by = " ")
?merge()
names(estimations.df)[names(estimations.df) == ''] <- 'id'
View(estimations.df)
file.choose()
estimations.df <- read.csv("/home/chris/Desktop/estimationsdf.csv")
View(estimations.df)
estimationstest.df <- read.csv("/home/chris/Desktop/estimationstest.csv")
View(estimationstest.df)
merge(estimations.df, estimationstest.df, by = "id")
estimations.merged <- merge(estimations.df, estimationstest.df, by = "id")
View(estimations.merged)
write.csv(estimations.merged, file ="/home/chris/Desktop/estimationsmerged.csv")
nrow(class.comparison)
write.csv(class.comparison, file ="/home/chris/Desktop/classcomparison.csv")
84/1029
1-0.08163265
