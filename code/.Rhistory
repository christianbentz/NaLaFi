# unigram entropy estimation
# calculate unigram entropy for characters
chars.df <- as.data.frame(table(chars[1:(i*stepsize)]))
# print(chars.df)
huni.chars <- entropy(chars.df$Freq, method = "ML", unit = "log2")
# entropy rate estimation
# note: the values chosen for max.length and every.word will crucially
# impact processing time. max.length = NULL means all units in the file are
# considered.
hrate.chars <- get.estimate(text = chars[1:(i*stepsize)], every.word = 1,
max.length = NULL)
# append results to dataframe
local.df <- data.frame(filename, subcorpus, code, huni.chars, hrate.chars,
units = i*stepsize)
stabilization.df <- rbind(stabilization.df, local.df)
}
# counter
counter <- counter + 1
# print(counter)
}
end_time <- Sys.time()
end_time - start_time
View(stabilization.df)
stabilization.df$hdiff <- stabilization.df$huni.chars - stabilization.df$hrate.chars
stabilization.df.long <- melt(stabilization.df, measure.vars = c("huni.chars", "hrate.chars"),
variable.name = "measure")
data.generated <- stabilization.df.long[stabilization.df.long$code == "combinatorial" |
stabilization.df.long$code == "random",]
# plot
entropy.generated.plot <- ggplot(data.generated, aes(x = units, y = value,
colour = measure)) +
geom_line(alpha = 0.8, size  = 1.5) +
theme(legend.position = "bottom") +
labs(x = "number of characters", y = "entropy") +
facet_wrap(~code)
entropy.generated.plot
data.natural <- stabilization.df.long[stabilization.df.long$code == "chinese" |
stabilization.df.long$code == "english" |
stabilization.df.long$code == "sumerian" |
stabilization.df.long$code == "paleolithic (deer teeth)" |
stabilization.df.long$code == "paleolithic (mammoth)",]
# plot
entropy.natural.plot <- ggplot(data.natural, aes(x = units, y = value,
colour = code, shape = measure)) +
geom_line(size  = 1) +
geom_point(size = 1, color = "black") +
#scale_linetype_manual(values = c("dashed", "dotted")) +
#theme(legend.position = "bottom") +
labs(x = "number of characters", y = "entropy") +
facet_wrap(~code)
entropy.natural.plot
View(stabilization.df.long)
file.list <- list.files(path = "/home/chris/Github/StringBase/corpus/generated/test",
recursive = T, full.names = T)
print(file.list)
length(file.list)
counter = 0
# set the maximal number of units (n), and the stepsize for stabilization analysis
# (i.e. in steps of how many units are values calculated?)
n = 500
stepsize = 10
# initialize dataframe to append results to
stabilization.df <- data.frame(filename = character(0), subcorpus = character(0),
code = character(0), huni.chars = numeric (0),
hrate.chars = numeric(0), units = numeric(0))
# start time
start_time <- Sys.time()
for (file in file.list)
{
# basic processing
# loading textfile
textfile <- scan(file, what = "char", quote = "",
comment.char = "", encoding = "UTF-8", sep = "\n" , skip = 7, nmax = 20)
# nmax is the maximum number of lines to be read
# remove tabs and parentheses
textfile <- gsubfn(".", list("\t" = "", "(" = "", ")" = "", "]" = "",
"[" = "", "}" = "",  "{" = ""), textfile)
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
# print(head(textfile))
# get filename
filename <- basename(file)
# print(filename) # for visual inspection
# get subcorpus category
subcorpus <- sub("_.*", "", filename)
# print(subcorpus) # for visual inspection
# get identification code by removing beginning and end of filename string
code <- sub(".txt", "", substring(filename, regexpr("_", filename) + 1))
# print(code) # for visual inspection
# split the textfile into individual utf-8 characters. The output of strsplit()
# is a list, so it needs to be "unlisted"" to get a vector. Note that white spaces    # are counted as utf-8 characters here.
chars <- unlist(strsplit(textfile, ""))
chars <- chars[1:n] # use only maximally n units
chars <- chars[!is.na(chars)] # remove NAs for vectors which are already shorter
# than n
chars <- chars[chars != " "] # remove white spaces from character vector
# define the number of units (i.e. characters) used for analyses (note that k is      # always either equal to or smaller than n)
k = length(chars)
for (i in 1:(k/stepsize))
{
# unigram entropy estimation
# calculate unigram entropy for characters
chars.df <- as.data.frame(table(chars[1:(i*stepsize)]))
# print(chars.df)
huni.chars <- entropy(chars.df$Freq, method = "ML", unit = "log2")
# entropy rate estimation
# note: the values chosen for max.length and every.word will crucially
# impact processing time. max.length = NULL means all units in the file are
# considered.
hrate.chars <- get.estimate(text = chars[1:(i*stepsize)], every.word = 1,
max.length = NULL)
# append results to dataframe
local.df <- data.frame(filename, subcorpus, code, huni.chars, hrate.chars,
units = i*stepsize)
stabilization.df <- rbind(stabilization.df, local.df)
}
# counter
counter <- counter + 1
# print(counter)
}
end_time <- Sys.time()
end_time - start_time
stabilization.df$hdiff <- stabilization.df$huni.chars - stabilization.df$hrate.chars
stabilization.df.long <- melt(stabilization.df, measure.vars = c("huni.chars", "hrate.chars"),
variable.name = "measure")
data.generated <- stabilization.df.long[stabilization.df.long$code == "combinatorial" |
stabilization.df.long$code == "random",]
# plot
entropy.generated.plot <- ggplot(data.generated, aes(x = units, y = value,
colour = measure)) +
geom_line(alpha = 0.8, size  = 1.5) +
theme(legend.position = "bottom") +
labs(x = "number of characters", y = "entropy") +
facet_wrap(~code)
entropy.generated.plot
data.natural <- stabilization.df.long[stabilization.df.long$code == "chinese" |
stabilization.df.long$code == "english" |
stabilization.df.long$code == "sumerian" |
stabilization.df.long$code == "paleolithic (deer teeth)" |
stabilization.df.long$code == "paleolithic (mammoth)",]
# plot
entropy.natural.plot <- ggplot(data.natural, aes(x = units, y = value,
colour = code, shape = measure)) +
geom_line(size  = 1) +
geom_point(size = 1, color = "black") +
#scale_linetype_manual(values = c("dashed", "dotted")) +
#theme(legend.position = "bottom") +
labs(x = "number of characters", y = "entropy") +
facet_wrap(~code)
entropy.natural.plot
data.natural <- stabilization.df.long[stabilization.df.long$code == "chinese" |
stabilization.df.long$code == "english" |
stabilization.df.long$code == "sumerian" |
stabilization.df.long$code == "paleolithic (deer teeth)" |
stabilization.df.long$code == "paleolithic (mammoth)",]
# plot
entropy.natural.plot <- ggplot(data.natural, aes(x = units, y = value,
colour = code, shape = measure)) +
geom_line(size  = 1) +
geom_point(size = 1, color = "black") +
#scale_linetype_manual(values = c("dashed", "dotted")) +
#theme(legend.position = "bottom") +
labs(x = "number of characters", y = "entropy") +
facet_wrap(~code)
entropy.natural.plot
entropy.diff.plot <- ggplot(data.natural, aes(x = units, y = hdiff,
colour = code)) +
geom_line(size  = 1) +
geom_point(size = 1, shape = 15, color = "black") +
theme(legend.position = "none") +
labs(x = "number of characters", y = "entropy difference (Î”E)")#+
#facet_wrap(~code)
entropy.diff.plot
data.original <- stabilization.df[stabilization.df$code == "chinese" |
stabilization.df$code == "combinatorial" |
stabilization.df$code == "english" |
stabilization.df$code == "sumerian" |
stabilization.df$code == "random" |
stabilization.df$code == "paleolithic (aurignacian)" |
stabilization.df$code == "paleolithic (deer teeth)" |
stabilization.df$code == "paleolithic (mammoth)",]
# select shuffled data
data.shuffled <- stabilization.df[stabilization.df$code == "chinese_shuffled" |
stabilization.df$code == "combinatorial_shuffled" |
stabilization.df$code == "english_shuffled" |
stabilization.df$code == "random_shuffled" |
stabilization.df$code == "sumerian_shuffled" |
stabilization.df$code == "paleolithic (aurignacian)_shuffled" |
stabilization.df$code == "paleolithic (deer teeth)_shuffled" |
stabilization.df$code == "paleolithic (mammoth)_shuffled",]
data.shuffled$code <- sub("_shuffled", "", data.shuffled$code)
# add columns with shuffling information
data.original$type <- rep("original", nrow(data.original))
data.shuffled$type <- rep("shuffled", nrow(data.shuffled))
# combine data frames
data.combined <- rbind(data.original, data.shuffled)
# plot
entropy.shuffled.plot <- ggplot(data.combined, aes(x = units, y = hrate.chars,
colour = code, shape = type)) +
geom_line(size  = 1) +
geom_point(size = 1, color = "black") +
#scale_linetype_manual(values = c("dashed", "dotted")) +
#theme(legend.position = "bottom") +
labs(x = "number of characters", y = "entropy") +
facet_wrap(~code)
entropy.shuffled.plot
entropy.shuffled.plot <- ggplot(data.combined, aes(x = units, y = hrate.chars,
colour = code, shape = type)) +
geom_line(size  = 1) +
geom_point(size = 1, color = "black") +
#scale_linetype_manual(values = c("dashed", "dotted")) +
#theme(legend.position = "bottom") +
labs(x = "number of characters", y = "entropy") +
facet_wrap(~code)
entropy.shuffled.plot
?knn()
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(class)
library(gridExtra)
library(gmodels)
library(caret)
?knn()
install.packages("markovchain")
knitr::opts_chunk$set(echo = TRUE)
sequence <- c("a", "b", "a", "a", "a", "a", "b", "a", "b", "a",
"b", "a", "a", "b", "b", "b", "a")
mcFit <- markovchainFit(data=sequence)
library(markovchain)
sequence <- c("a", "b", "a", "a", "a", "a", "b", "a", "b", "a",
"b", "a", "a", "b", "b", "b", "a")
mcFit <- markovchainFit(data=sequence)
mcFit
sequence <- c("a", "a", "a", "a", "a", "b", "b", "b", "b", "b", "b")
mcFit <- markovchainFit(data=sequence)
mcFit
?markovChainFit()
markovchainFit()
markovchainFit
?markovchainFit()
sequence <- c("a", "a", "a", "a", "a", "b", "b", "b", "b", "b", "b")
mcFit <- fitHigherOrder(data=sequence, order = 1)
sequence <- c("a", "a", "a", "a", "a", "b", "b", "b", "b", "b", "b")
mcFit <- fitHigherOrder(data=sequence, order = 2)
sequence <- c("a", "a", "a", "a", "a", "b", "b", "b", "b", "b", "b")
mcFit <- fitHigherOrder(sequence, order = 2)
sequence <- c("a", "a", "a", "a", "a", "b", "b", "b", "b", "b", "b")
mcFit <- markovchainFit(sequence, order = 2)
sequence <- c("a", "a", "a", "a", "a", "b", "b", "b", "b", "b", "b")
mcFit <- markovchainFit(sequence)
mcFit
sequence <- c("a", "a", "a", "a", "a", "b", "b", "b", "b", "b")
mcFit <- markovchainFit(sequence)
mcFit
file.choose()
file <- "~/Github/StringBase/corpus/original/paleolithic/paleolithic_vhc_0145.txt"
textfile <- scan(file, what = "char", quote = "", comment.char = "",
encoding = "UTF-8", sep = "\n" , skip = 7)
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
textfile
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",  "\\{" = ""))
install.packages("stringr")
library(markovchain)
library(stringr)
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",  "\\{" = ""))
textfile
strsplit(textfile, "")
unlist(strsplit(textfile, ""))
char.sequence <- unlist(strsplit(textfile, ""))
char.sequence <- unlist(strsplit(textfile, ""))
markov.fitted <- markovchainFit(char.sequence)
print(markov.fitted)
choose.files()
file.choose()
file <- "~/Github/StringBase/corpus/original/paleolithic/paleolithic_sgr_0001.txt"
textfile <- scan(file, what = "char", quote = "", comment.char = "",
encoding = "UTF-8", sep = "\n" , skip = 7)
# preprocessing
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",  "\\{" = ""))
char.sequence <- unlist(strsplit(textfile, ""))
char.sequence
markov.fitted <- markovchainFit(char.sequence)
print(markov.fitted)
file.choose()
file <- "~/Github/StringBase/corpus/original/writing/udhr/writing_eng_0001.txt"
textfile <- scan(file, what = "char", quote = "", comment.char = "",
encoding = "UTF-8", sep = "\n" , skip = 7, nmax = 100)
# preprocessing
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",  "\\{" = ""))
char.sequence <- unlist(strsplit(textfile, ""))
char.sequence
char.sequence[1:100]
markov.fitted <- markovchainFit(char.sequence[1:100])
print(markov.fitted)
file <- "~/Github/StringBase/corpus/original/paleolithic/paleolithic_vhc_0145.txt"
textfile <- scan(file, what = "char", quote = "", comment.char = "",
encoding = "UTF-8", sep = "\n" , skip = 7)
# preprocessing
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",  "\\{" = ""))
char.sequence <- unlist(strsplit(textfile, ""))
length(char.sequence)
file <- "~/Github/StringBase/corpus/original/writing/udhr/writing_eng_0001.txt"
textfile <- scan(file, what = "char", quote = "", comment.char = "",
encoding = "UTF-8", sep = "\n" , skip = 7)
# preprocessing
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",  "\\{" = ""))
char.sequence <- unlist(strsplit(textfile, ""))
char.sequence
file.choose()
file <-  "~/Github/StringBase/corpus/generated/test/original/test_english_1stArticle.txt"
textfile <- scan(file, what = "char", quote = "", comment.char = "",
encoding = "UTF-8", sep = "\n" , skip = 7)
# preprocessing
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",  "\\{" = ""))
char.sequence <- unlist(strsplit(textfile, ""))
char.sequence
char.sequence[1:57]
markov.fitted <- markovchainFit(char.sequence[1:57])
print(markov.fitted)
char.sequence[1:15]
char.sequence[1:16]
markov.fitted <- markovchainFit(char.sequence[1:16])
print(markov.fitted)
char.sequence[1:10]
markov.fitted <- markovchainFit(char.sequence[1:10])
print(markov.fitted)
char.sequence <- c("G","o","n","e","."," ","G","o","n","e",".")
markov.fitted <- markovchainFit(char.sequence)
print(markov.fitted)
file <- "~/Github/StringBase/corpus/original/paleolithic/paleolithic_vhc_0145.txt"
file <- "~/Github/StringBase/corpus/original/paleolithic/paleolithic_vhc_0145.txt"
#file <- "~/Github/StringBase/corpus/original/paleolithic/paleolithic_sgr_0001.txt"
#file <-  "~/Github/StringBase/corpus/generated/test/original/test_english_1stArticle.txt"
# read sequence into R
textfile <- scan(file, what = "char", quote = "", comment.char = "",
encoding = "UTF-8", sep = "\n" , skip = 7)
# preprocessing
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",  "\\{" = ""))
char.sequence <- unlist(strsplit(textfile, ""))
file <- "~/Github/StringBase/corpus/original/paleolithic/paleolithic_vhc_0145.txt"
#file <- "~/Github/StringBase/corpus/original/paleolithic/paleolithic_sgr_0001.txt"
#file <-  "~/Github/StringBase/corpus/generated/test/original/test_english_1stArticle.txt"
# read sequence into R
textfile <- scan(file, what = "char", quote = "", comment.char = "",
encoding = "UTF-8", sep = "\n" , skip = 7)
# preprocessing
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",  "\\{" = ""))
char.sequence <- unlist(strsplit(textfile, ""))
markov.fitted <- markovchainFit(char.sequence)
print(markov.fitted)
0.1470588+0.8235294+0.02941176
knitr::opts_chunk$set(echo = TRUE)
library(markovchain)
library(stringr)
char.sequence <- c("G","o","n","e","."," ","G","o","n","e",".")
markov.fitted <- markovchainFit(char.sequence)
print(markov.fitted)
char.sequence <- c("G","o","n","e"," "," ","G","o","n","e"," ")
markov.fitted <- markovchainFit(char.sequence)
print(markov.fitted)
char.sequence <- c("G","o","n","e","."," ","G","o","n","e",".")
markov.fitted <- markovchainFit(char.sequence)
print(markov.fitted)
char.sequence <- c(" ","G","o","n","e"," ","G","o","n","e"," ")
markov.fitted <- markovchainFit(char.sequence)
print(markov.fitted)
print(markov.fitted)
char.sequence <- c("G","i","t","t","i"," ","G","i","t","t","i"," ")
markov.fitted <- markovchainFit(char.sequence)
print(markov.fitted)
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
library(ggplot2)
library(plyr)
library(entropy)
install.packages("entropy")
library(stringr)
library(ggplot2)
library(plyr)
library(entropy)
library(ggExtra)
library("ggExtra")
install.packages("ggExtra")
library(ggExtra)
library(ggpubr)
install.packages("ggpubr")
library(gsubfn)
install.packages("gsubfn")
library(gsubfn)
library(devtools)
install.packages("devtools")
install.packages("reshape2")
library(reshape2)
library(ggExtra)
library(ggpubr)
install.packages("ggpubr")
install.packages("libcurl")
install.packages("ggpubr")
library(ggpubr)
install.packages("devtools")
install.packages("openssl")
library(devtools)
install.packages("devtools")
install.packages("devtools")
library(devtools)
install_github("dimalik/Hrate")
library(Hrate)
library(reshape2)
file.list <- list.files(path = "/home/chris/Github/StringBase/corpus/generated/test",
recursive = T, full.names = T)
print(file.list)
length(file.list)
counter = 0
# set the maximal number of units (n), and the stepsize for stabilization analysis
# (i.e. in steps of how many units are values calculated?)
n = 100
stepsize = 10
# initialize dataframe to append results to
stabilization.df <- data.frame(filename = character(0), subcorpus = character(0),
code = character(0), huni.chars = numeric (0),
hrate.chars = numeric(0), units = numeric(0))
start_time <- Sys.time()
for (file in file.list)
{
# basic processing
# loading textfile
textfile <- scan(file, what = "char", quote = "",
comment.char = "", encoding = "UTF-8", sep = "\n" , skip = 7) #nmax = 100
# nmax is the maximum number of lines to be read
# remove tabs and parentheses
textfile <- gsubfn(".", list("\t" = "", "(" = "", ")" = "", "]" = "",
"[" = "", "}" = "",  "{" = ""), textfile)
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
# print(head(textfile))
# get filename
filename <- basename(file)
# print(filename) # for visual inspection
# get subcorpus category
subcorpus <- sub("_.*", "", filename)
# print(subcorpus) # for visual inspection
# get identification code by removing beginning and end of filename string
code <- sub(".txt", "", substring(filename, regexpr("_", filename) + 1))
# print(code) # for visual inspection
# split the textfile into individual utf-8 characters. The output of strsplit()
# is a list, so it needs to be "unlisted"" to get a vector. Note that white spaces
# are counted as utf-8 characters here.
chars <- unlist(strsplit(textfile, ""))
chars <- chars[1:n] # use only maximally n units
chars <- chars[!is.na(chars)] # remove NAs for vectors which are already shorter
# than n
# chars <- chars[chars != " "] # remove white spaces from character vector
# define the number of units (i.e. characters) used for analyses (note that k is
# always either equal to or smaller than n)
k = length(chars)
for (i in 1:(k/stepsize))
{
# unigram entropy estimation
# calculate unigram entropy for characters
chars.df <- as.data.frame(table(chars[1:(i*stepsize)]))
# print(chars.df)
huni.chars <- entropy(chars.df$Freq, method = "ML", unit = "log2")
# entropy rate estimation
# note: the values chosen for max.length and every.word will crucially
# impact processing time. max.length = NULL means all units in the file are
# considered.
hrate.chars <- get.estimate(text = chars[1:(i*stepsize)], every.word = 1,
max.length = NULL)
# append results to dataframe
local.df <- data.frame(filename, subcorpus, code, huni.chars, hrate.chars,
units = i*stepsize)
stabilization.df <- rbind(stabilization.df, local.df)
}
# counter
counter <- counter + 1
# print(counter)
}
end_time <- Sys.time()
end_time - start_time
head(stabilization.df)
unique(stabilization.df$code)
knitr::opts_chunk$set(echo = TRUE)
library(markovchain)
library(stringr)
char.sequence <- c("G","i","t","t","i"," ","G","i","t","t","i"," ")
markov.fitted <- markovchainFit(char.sequence)
print(markov.fitted)
