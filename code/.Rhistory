View(estimations.df)
View(estimations.df)
View(estimations.df)
estimations.df <- read.csv("~/Github/NaLaFi/results/tables/estimations.csv")
head(estimations.df)
View(estimations.df)
View(estimations.df)
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
library(ggplot2)
library(plyr)
library(entropy)
library(ggExtra)
library(gsubfn)
# library(devtools)
# install_github("dimalik/Hrate")
library(Hrate)
file.list <- list.files(path = "~/Github/NaLaFi/data/",
recursive = T, full.names = T)
head(file.list)
length(file.list)
knitr::opts_chunk$set(echo = TRUE)
file.list <- list.files(path = "~/Github/NaLaFi/data/",
recursive = T, full.names = T)
head(file.list)
length(file.list)
counter = 0
# set the maximal number of units (n) to be used for analysis
n = 1000
# initialize dataframe to append results to
estimations.df <- data.frame(filename = character(0), subcorpus = character(0),
code = character(0), huni.chars = numeric (0),
huni.strings = numeric(0), hrate.chars = numeric(0),
hrate.strings = numeric(0), ttr.chars = numeric(0),
ttr.strings = numeric (0), rm.chars = numeric(0))
# start time
start_time <- Sys.time()
for (file in file.list)
{
# basic processing
# loading textfile
textfile <- scan(file, what = "char", quote = "", comment.char = "",
encoding = "UTF-8", sep = "\n" , skip = 7)
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
# print(head(textfile))
# get filename
filename <- basename(file)
print(filename) # for visual inspection
# get subcorpus category
subcorpus <- sub("_.*", "", filename)
# print(subcorpus) # for visual inspection
# get the three letter identification code + the running number
code <- substring(substring(filename, regexpr("_", filename) + 1), 1, 8)
print(code) # for visual inspection
# Split into individual characters/signs
# remove tabs and parentheses, as well as star signs `*' and plus signs `+´
# note that this might have to be tuned according to the text files included
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",
"\\{" = "", "\\*" = "", "\\+" = ""))
# split the textfile into individual utf-8 characters. Note that white spaces are
# counted as utf-8 characters here.
chars <- unlist(strsplit(textfile, ""))
chars <- chars[1:n] # use only maximally n units
chars <- chars[!is.na(chars)] # remove NAs for vectors which are already shorter
# than n
# chars <- chars[chars != " "] # remove white spaces from character vector
# Split the textfile into strings delimited by white spaces. The output of strsplit()
# is a list, so it needs to be "unlisted"" to get a vector.
strings <- unlist(strsplit(textfile, " "))
strings <- strings[1:n] # use only maximally n units
strings <- strings[!is.na(strings)] # remove NAs for vectors which are already
# shorter than n
# Unigram entropy estimation
# calculate unigram entropy for characters
chars.df <- as.data.frame(table(chars))
# print(chars.df)
huni.chars <- entropy(chars.df$Freq, method = "ML", unit = "log2")
# calculate unigram entropy for strings (Maximum Likelihood method)
strings.df <- as.data.frame(table(strings))
# print(strings.df)
huni.strings <- entropy(strings.df$Freq, method = "ML", unit = "log2")
# entropy rate estimation
# the values chosen for max.length and every.word will crucially
# impact processing time. In case of "max.length = NULL" the length is n units
hrate.chars <- get.estimate(text = chars, every.word = 1, max.length = NULL)
hrate.strings <- get.estimate(text = strings, every.word = 1, max.length = NULL)
# calculate type-token ratio (ttr)
ttr.chars <- nrow(chars.df)/sum(chars.df$Freq)
ttr.strings <- nrow(strings.df)/sum(strings.df$Freq)
# calculate repetition measure according to Sproat (2014)
# the overall number of repetitions is the sum of frequency counts minus 1.
R <- sum(chars.df$Freq-1)
# calculate the number of adjacent repetitions
r = 0
if (length(chars) > 1){
for (i in 1:(length(chars)-1)){
if (chars[i] == chars[i+1]){
r = r + 1
} else {
r = r + 0
}
}
# calculate the repetition measure
rm.chars <- r/R
} else {
rm.chars <- "NA"
}
# append results to dataframe
local.df <- data.frame(filename, subcorpus, code, huni.chars,
hrate.chars, huni.strings, hrate.strings,
ttr.chars, ttr.strings, rm.chars)
estimations.df <- rbind(estimations.df, local.df)
# counter
counter <- counter + 1
# print(counter)
}
counter = 0
# set the maximal number of units (n) to be used for analysis
n = 1000
# initialize dataframe to append results to
estimations.df <- data.frame(filename = character(0), subcorpus = character(0),
code = character(0), huni.chars = numeric (0),
huni.strings = numeric(0), hrate.chars = numeric(0),
hrate.strings = numeric(0), ttr.chars = numeric(0),
ttr.strings = numeric (0), rm.chars = numeric(0))
# start time
start_time <- Sys.time()
for (file in file.list)
{
try({
# basic processing
# loading textfile
textfile <- scan(file, what = "char", quote = "", comment.char = "",
encoding = "UTF-8", sep = "\n" , skip = 7)
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
# print(head(textfile))
# get filename
filename <- basename(file)
print(filename) # for visual inspection
# get subcorpus category
subcorpus <- sub("_.*", "", filename)
# print(subcorpus) # for visual inspection
# get the three letter identification code + the running number
code <- substring(substring(filename, regexpr("_", filename) + 1), 1, 8)
# Split into individual characters/signs
# remove tabs and parentheses, as well as star signs `*' and plus signs `+´
# note that this might have to be tuned according to the text files included
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",
"\\{" = "", "\\*" = "", "\\+" = ""))
# split the textfile into individual utf-8 characters. Note that white spaces are
# counted as utf-8 characters here.
chars <- unlist(strsplit(textfile, ""))
chars <- chars[1:n] # use only maximally n units
chars <- chars[!is.na(chars)] # remove NAs for vectors which are already shorter
# than n
# chars <- chars[chars != " "] # remove white spaces from character vector
# Split the textfile into strings delimited by white spaces. The output of strsplit()
# is a list, so it needs to be "unlisted"" to get a vector.
strings <- unlist(strsplit(textfile, " "))
strings <- strings[1:n] # use only maximally n units
strings <- strings[!is.na(strings)] # remove NAs for vectors which are already
# shorter than n
# Unigram entropy estimation
# calculate unigram entropy for characters
chars.df <- as.data.frame(table(chars))
# print(chars.df)
huni.chars <- entropy(chars.df$Freq, method = "ML", unit = "log2")
# calculate unigram entropy for strings (Maximum Likelihood method)
strings.df <- as.data.frame(table(strings))
# print(strings.df)
huni.strings <- entropy(strings.df$Freq, method = "ML", unit = "log2")
# entropy rate estimation
# the values chosen for max.length and every.word will crucially
# impact processing time. In case of "max.length = NULL" the length is n units
hrate.chars <- get.estimate(text = chars, every.word = 1, max.length = NULL)
hrate.strings <- get.estimate(text = strings, every.word = 1, max.length = NULL)
# calculate type-token ratio (ttr)
ttr.chars <- nrow(chars.df)/sum(chars.df$Freq)
ttr.strings <- nrow(strings.df)/sum(strings.df$Freq)
# calculate repetition measure according to Sproat (2014)
# the overall number of repetitions is the sum of frequency counts minus 1.
R <- sum(chars.df$Freq-1)
# calculate the number of adjacent repetitions
r = 0
if (length(chars) > 1){
for (i in 1:(length(chars)-1)){
if (chars[i] == chars[i+1]){
r = r + 1
} else {
r = r + 0
}
}
# calculate the repetition measure
rm.chars <- r/R
} else {
rm.chars <- "NA"
}
# append results to dataframe
local.df <- data.frame(filename, subcorpus, code, huni.chars,
hrate.chars, huni.strings, hrate.strings,
ttr.chars, ttr.strings, rm.chars)
estimations.df <- rbind(estimations.df, local.df)
# counter
counter <- counter + 1
# print(counter)
})
}
end_time <- Sys.time()
end_time - start_time
write.csv(estimations.df, "~/Github/NaLaFi/results/tables/estimations.csv", row.names = F)
?paste()
n=100
paste("number", n)
paste("number", n, sep="")
paste("number", paste(n, ".csv", sep=""), sep="")
file.list <- list.files(path = "~/Github/NaLaFi/data/",
recursive = T, full.names = T)
head(file.list)
length(file.list)
counter = 0
# set the maximal number of units (n), and the stepsize for stabilization analysis
# (i.e. in steps of how many units are values calculated?)
n = 100
stepsize = 10
# initialize dataframe to append results to
stabilization.df <- data.frame(filename = character(0), subcorpus = character(0),
code = character(0), huni.chars = numeric (0),
hrate.chars = numeric(0), ttr = numeric(0),
units = numeric(0))
# start time
start_time <- Sys.time()
for (file in file.list)
{
try({ # if the processing failes for a certain file, there will be no output for this file,
# but the try() function allows the loop to keep running
# basic processing
# loading textfile
textfile <- scan(file, what = "char", quote = "",
comment.char = "", encoding = "UTF-8", sep = "\n" , skip = 7, nmax = 20) # skip 7 first lines
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
# print(head(textfile))
# get filename
filename <- basename(file)
#print(filename) # for visual inspection
# get subcorpus category
subcorpus <- sub("_.*", "", filename)
# print(subcorpus) # for visual inspection
# get the three letter identification code + the running number
code <- substring(substring(filename, regexpr("_", filename) + 1), 1, 8)
# Split into individual characters/signs
# remove tabs and parentheses, as well as star signs `*' and plus signs `+´
# note that this might have to be tuned according to the text files included
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",
"\\{" = "", "\\*" = "", "\\+" = ""))
# split the textfile into individual utf-8 characters. Note that white spaces are
# counted as utf-8 characters here.
chars <- unlist(strsplit(textfile, ""))
chars <- chars[1:n] # use only maximally n units
chars <- chars[!is.na(chars)] # remove NAs for vectors which are already shorter
# than n
# chars <- chars[chars != " "] # remove white spaces from character vector
# run loop with stepsizes
# define the number of units (i.e. characters) used for analyses (note that k is
# always either equal to or smaller than n)
k = length(chars)
for (i in 1:(k/stepsize))
{
# unigram entropy estimation
# calculate unigram entropy for characters
chars.df <- as.data.frame(table(chars[1:(i*stepsize)]))
# print(chars.df)
huni.chars <- entropy(chars.df$Freq, method = "ML", unit = "log2")
# entropy rate estimation
# note: the values chosen for max.length and every.word will crucially
# impact processing time. max.length = NULL means all units in the file are
# considered.
hrate.chars <- get.estimate(text = chars[1:(i*stepsize)], every.word = 1,
max.length = NULL)
# calculate type-token ratio (ttr)
ttr.chars <- nrow(chars.df)/sum(chars.df$Freq)
# append results to dataframe
local.df <- data.frame(filename, subcorpus, code, huni.chars, hrate.chars,
ttr.chars, units = i*stepsize)
stabilization.df <- rbind(stabilization.df, local.df)
}
# counter
counter <- counter + 1
# print(counter)
})
}
end_time <- Sys.time()
end_time - start_time
huni.chars.plot <- ggplot(stabilization.df, aes(x = units, y = huni.chars,
colour = subcorpus)) +
geom_line(alpha = 0.8, size  = 1.5) +
theme(legend.position = "bottom") +
labs(x = "number of characters", y = "unigram entropy for characters") +
facet_wrap(~code)
huni.chars.plot
ggsave("~Github/NaLaFi/figures/stabilization_huni_chars.pdf", huni.chars.plot, dpi = 300,
scale = 1, device = cairo_pdf)
ggsave("~/Github/NaLaFi/figures/stabilization_huni_chars.pdf", huni.chars.plot, dpi = 300,
scale = 1, device = cairo_pdf)
ggsave("~/Github/NaLaFi/figures/stabilization_huni_chars.pdf", huni.chars.plot, dpi = 300,
scale = 1, device = cairo_pdf)
knitr::opts_chunk$set(echo = TRUE)
counter = 0
# set the maximal number of units (n), and the stepsize for stabilization analysis
# (i.e. in steps of how many units are values calculated?)
n = 100
stepsize = 10
# initialize dataframe to append results to
stabilization.df <- data.frame(filename = character(0), subcorpus = character(0),
code = character(0), huni.chars = numeric (0),
hrate.chars = numeric(0), ttr.chars = numeric(0),
rm.chars = numeric(0), units = numeric(0))
textfile <- scan(file, what = "char", quote = "",
comment.char = "", encoding = "UTF-8", sep = "\n" , skip = 7, nmax = 20)
# skip 7 first lines, nmax gives the maximum number of lines to be read,
# note that reading more lines will considerably increase processing time.
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
# print(head(textfile))
# get filename
filename <- basename(file)
#print(filename) # for visual inspection
# get subcorpus category
subcorpus <- sub("_.*", "", filename)
# print(subcorpus) # for visual inspection
# get the three letter identification code + the running number
code <- substring(substring(filename, regexpr("_", filename) + 1), 1, 8)
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",
"\\{" = "", "\\*" = "", "\\+" = ""))
library(stringr)
library(ggplot2)
library(plyr)
library(entropy)
library(ggExtra)
library(gsubfn)
# library(devtools)
# install_github("dimalik/Hrate")
library(Hrate)
textfile <- scan(file, what = "char", quote = "",
comment.char = "", encoding = "UTF-8", sep = "\n" , skip = 7, nmax = 20)
# skip 7 first lines, nmax gives the maximum number of lines to be read,
# note that reading more lines will considerably increase processing time.
# remove annotations marked by '<>'
textfile <- gsub("<.*>","",textfile)
# print(head(textfile))
# get filename
filename <- basename(file)
#print(filename) # for visual inspection
# get subcorpus category
subcorpus <- sub("_.*", "", filename)
# print(subcorpus) # for visual inspection
# get the three letter identification code + the running number
code <- substring(substring(filename, regexpr("_", filename) + 1), 1, 8)
# Split into individual characters/signs
# remove tabs and parentheses, as well as star signs `*' and plus signs `+´
# note that this might have to be tuned according to the text files included
textfile <- str_replace_all(textfile, c("\\\t" = "", "\\(" = "", "\\)" = "",
"\\]" = "", "\\[" = "",  "\\}" = "",
"\\{" = "", "\\*" = "", "\\+" = ""))
# split the textfile into individual utf-8 characters. Note that white spaces are
# counted as utf-8 characters here.
chars <- unlist(strsplit(textfile, ""))
chars <- chars[1:n] # use only maximally n units
chars <- chars[!is.na(chars)] # remove NAs for vectors which are already shorter
# than n
# chars <- chars[chars != " "] # remove white spaces from character vector
chars
k = length(chars)
k
1:(k/stepsize)
i=10
chars.df <- as.data.frame(table(chars[1:(i*stepsize)]))
# print(chars.df)
huni.chars <- entropy(chars.df$Freq, method = "ML", unit = "log2")
# entropy rate estimation
# note: the values chosen for max.length and every.word will crucially
# impact processing time. max.length = NULL means all units in the file are
# considered.
hrate.chars <- get.estimate(text = chars[1:(i*stepsize)], every.word = 1,
max.length = NULL)
# calculate type-token ratio (ttr)
ttr.chars <- nrow(chars.df)/sum(chars.df$Freq)
chars.df
ttr.chars
R <- sum(chars.df$Freq-1)
# calculate the number of adjacent repetitions
r = 0
if (length(chars) > 1){
for (i in 1:(length(chars)-1)){
if (chars[i] == chars[i+1]){
r = r + 1
} else {
r = r + 0
}
}
# calculate the repetition measure
rm.chars <- r/R
} else {
rm.chars <- "NA"
}
rm.chars
local.df <- data.frame(filename, subcorpus, code, huni.chars, hrate.chars,
ttr.chars, rm.chars, units = i*stepsize)
local.df
i
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
library(ggplot2)
library(ggrepel)
library(plyr)
library(ggExtra)
library(ggpubr)
estimations10.df <- read.csv("~/Github/NaLaFi/results/estimation10chars.csv")
View(estimations10.df)
estimations10.df <- read.csv("~/Github/NaLaFi/results/estimation10chars.csv")
estimations100.df <- read.csv("~/Github/NaLaFi/results/estimation100chars.csv")
estimations1000.df <- read.csv("~/Github/NaLaFi/results/estimation1000chars.csv")
huni.hrate.chars.plot <- ggplot(estimations10.df,
aes(x = huni.chars, y = hrate.chars,
colour = subcorpus)) +
geom_point(alpha = 0.5, size  = 1) +
# geom_smooth(method = "lm") +
xlim(0, max(estimations.df.combined$huni.chars)) +
ylim(0, max(estimations.df.combined$hrate.chars)) +
labs(x = "Unigram entropy for characters", y = "Entropy rate for characters") +
theme(legend.position = "none")
huni.hrate.chars.plot <- ggMarginal(huni.hrate.chars.plot, groupFill = T, type = "histogram", colour = "white")
huni.hrate.chars.plot
huni.hrate.chars.plot <- ggplot(estimations10.df,
aes(x = huni.chars, y = hrate.chars,
colour = subcorpus)) +
geom_point(alpha = 0.5, size  = 1) +
# geom_smooth(method = "lm") +
xlim(0, max(estimations10.df$huni.chars)) +
ylim(0, max(estimations10.df$hrate.chars)) +
labs(x = "Unigram entropy for characters", y = "Entropy rate for characters") +
theme(legend.position = "none")
huni.hrate.chars.plot <- ggMarginal(huni.hrate.chars.plot, groupFill = T, type = "histogram", colour = "white")
huni.hrate.chars.plot
huni.hrate.chars.plot <- ggplot(estimations10.df,
aes(x = huni.chars, y = hrate.chars,
colour = subcorpus)) +
geom_point(alpha = 0.5, size  = 1) +
xlim(0, max(estimations10.df$huni.chars)) +
ylim(0, max(estimations10.df$hrate.chars)) +
labs(x = "Unigram entropy for characters", y = "Entropy rate for characters") +
theme(legend.position = "right")
huni.hrate.chars.plot <- ggMarginal(huni.hrate.chars.plot, groupFill = T, type = "histogram", colour = "white")
huni.hrate.chars.plot
ttr.rm.chars.plot <- ggplot(estimations10.df,
aes(x = ttr.chars, y = rm.chars,
colour = subcorpus)) +
geom_point(alpha = 0.5, size  = 1) +
theme(legend.position = "left") +
labs(x = "TTR for characters", y = "Repetition rate for characters")
ttr.rm.chars.plot <- ggMarginal(ttr.rm.chars.plot, groupFill = T, type = "histogram", colour = "white")
ttr.rm.chars.plot
View(estimations10.df)
estimations1000.df <- read.csv("~/Github/NaLaFi/results/estimation1000chars.csv")
selected <- c("shuffled")
estimations1000.df <- estimations1000.df[estimations1000.df$subcorpus %notin% selected, ]
selected <- c("shuffled")
estimations1000.df <- estimations1000.df[estimations1000.df$subcorpus !%in% selected, ]
selected <- c("shuffled")
estimations1000.df <- estimations1000.df[!(estimations1000.df$subcorpus %in% selected), ]
knitr::opts_chunk$set(echo = TRUE)
estimations10.df <- read.csv("~/Github/NaLaFi/results/estimation10chars.csv")
estimations100.df <- read.csv("~/Github/NaLaFi/results/estimation100chars.csv")
estimations1000.df <- read.csv("~/Github/NaLaFi/results/estimation1000chars.csv")
View(estimations10.df)
View(estimations10.df)
huni.hrate.chars.plot <- ggplot(estimations1000.df,
aes(x = huni.chars, y = ttr.chars,
colour = subcorpus)) +
geom_point(alpha = 0.5, size  = 1) +
labs(x = "Unigram entropy for characters", y = "TTR for characters") +
facet_wrap(~ subcorpus, ncol = 2) +
theme(legend.position = "none")
library(stringr)
library(ggplot2)
library(ggrepel)
library(plyr)
library(ggExtra)
library(ggpubr)
huni.hrate.chars.plot <- ggplot(estimations1000.df,
aes(x = huni.chars, y = ttr.chars,
colour = subcorpus)) +
geom_point(alpha = 0.5, size  = 1) +
labs(x = "Unigram entropy for characters", y = "TTR for characters") +
facet_wrap(~ subcorpus, ncol = 2) +
theme(legend.position = "none")
huni.hrate.chars.plot
huni.hrate.chars.plot <- ggplot(estimations1000.df,
aes(x = huni.chars, y = ttr.chars,
colour = subcorpus)) +
geom_point(alpha = 0.5, size  = 1) +
labs(x = "Unigram entropy for characters", y = "TTR for characters") +
facet_wrap(~ subcorpus, ncol = 3) +
theme(legend.position = "none")
#huni.hrate.chars.plot <- ggMarginal(huni.hrate.chars.plot, groupFill = T, type = "histogram", colour = "white")
huni.hrate.chars.plot
