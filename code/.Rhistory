linear.output = FALSE,
algorithm = algorithm, # defaults to "rprop+",
# i.e. resilient backpropagation
learningrate = 0.1,
err.fct = err.fct,
act.fct = act.fct,
likelihood = TRUE,
lifesign = 'minimal')
#classifier.mlp
# results matrix (each column represents one repetition)
# classifier.mlp$result.matrix
# Prediction
# Get prediction using the predict() function
mlp.predictions <- predict(classifier.mlp, test,
rep = which.min(classifier.mlp$result.matrix[1,]), # Predict response values
# based on the "best" repetition (epoche), i.e. the one with the lowest error.
all.units = FALSE)
# assign a label according to the rule that the label is "writing"
# if the prediction probability is >0.5, else assign "non-writing".
mlp.predictions.rd <- ifelse(mlp.predictions > 0.5, "writing", "non-writing")
#head(mlp.predictions.rd, 10)
#table(test$corpus == "non-writing", predictions[, 1] > 0.5)
# Model Evaluation
# creating a dataframe from known (true) test labels
test.labels <- data.frame(test$corpus)
# combining predicted and known classes
class.comparison <- data.frame(mlp.predictions.rd, test.labels)
# giving appropriate column names
names(class.comparison) <- c("predicted", "observed")
# inspecting our results table
head(class.comparison)
# get confusion matrix
cm <- confusionMatrix(as.factor(class.comparison$predicted),
reference = as.factor(class.comparison$observed))
#print(cm)
# get precision, recall, and f1 from the output list of confusionMatrix()
accuracy <- cm$overall['Accuracy']
f1 <- cm[["byClass"]]["F1"]
recall <- cm[["byClass"]]["Recall"]
precision <- cm[["byClass"]]["Precision"]
# unname and round resulting values
accuracy <- round(unname(accuracy), 2)
f1 <- round(unname(f1), 2)
recall <- round(unname(recall), 2)
precision <- round(unname(precision), 2)
# append results to dataframe
hidden.structure <- paste(unlist(hidden), collapse = ",")
hidden.size <- sum(unlist(hidden))
hidden.depth <- length(hidden)
local.df <- data.frame(num.char, hidden.structure, hidden.size,
hidden.depth, err.fct, act.fct.name, algorithm,
accuracy, precision, recall, f1)
results.df <- rbind(results.df, local.df)
})
counter <- counter + 1
print(counter)
}
end_time <- Sys.time()
proc.time <- end_time - start_time
print(proc.time)
write.csv(results.df, file = paste(c("~/Github/NaLaFi/results/MLP/results_MLP_",
num.char, "chars", "_",
act.fct.name, "_",
err.fct, "_",
algorithm,
".csv"
), collapse = ""),
row.names = F)
knitr::opts_chunk$set(echo = TRUE)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 10
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
nrow(estimations.df)
estimations.subset <- estimations.df[c("corpus", "subcorpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1:2], scale(estimations.subset[3:ncol(estimations.subset)]))
nrow(estimations.scaled)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 1000
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 1000
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
estimations.subset <- estimations.df[c("corpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1], scale(estimations.subset[2:ncol(estimations.subset)]))
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
train <- estimations.scaled[datasample == 1, 1:ncol(estimations.scaled)]
# Generate test set
test <- estimations.scaled[datasample == 2, 1:ncol(estimations.scaled)]
hidden.list <- list(c(4,2))
# initialize dataframe to append results to
results.df <- data.frame(num.char = numeric(0), hidden.structure = character(0),
hidden.size = numeric(0), hidden.depth = numeric(0),
err.fct = character(0), act.fct.name = character(0),
algorithm = character(0), accuracy = numeric(0),
precision = numeric(0), recall = numeric(0),
f1 = numeric(0)
)
set.seed(123)
# counter
counter <- 0
# start time
start_time <- Sys.time()
for (hidden in hidden.list) {
# Training
# choose error function
err.fct = 'ce' # options: 'sse', 'ce'
# choose activation function
act.fct = 'logistic' # options: 'logistic', 'tanh', softplus, relu
# note: softplus, relu, and tanh only work with sse as err.fct
act.fct.name = 'logistic' # make sure to give name as character string (for later storage)
# choose algorithm
algorithm = 'rprop+' # options: 'rprop+', 'rprop-', 'backprop', 'sag', 'slr'
# train classifier with neuralnet() function
try({ # if the processing failes for a certain file, there will be no output for this file,
# but the try() function allows the loop to keep running
classifier.mlp <- neuralnet(corpus == "writing" ~ .,
data = train,
hidden = hidden,
threshold = 0.1, # defaults to 0.01
rep = 1, # number of reps in which new initial values are used,
# (essentially the same as a for loop)
stepmax = 100000, # defaults to 100K
linear.output = FALSE,
algorithm = algorithm, # defaults to "rprop+",
# i.e. resilient backpropagation
# learningrate = 0.1,
err.fct = err.fct,
act.fct = act.fct,
likelihood = TRUE,
lifesign = 'minimal')
#classifier.mlp
# results matrix (each column represents one repetition)
# classifier.mlp$result.matrix
# Prediction
# Get prediction using the predict() function
mlp.predictions <- predict(classifier.mlp, test,
rep = which.min(classifier.mlp$result.matrix[1,]), # Predict response values
# based on the "best" repetition (epoche), i.e. the one with the lowest error.
all.units = FALSE)
# assign a label according to the rule that the label is "writing"
# if the prediction probability is >0.5, else assign "non-writing".
mlp.predictions.rd <- ifelse(mlp.predictions > 0.5, "writing", "non-writing")
#head(mlp.predictions.rd, 10)
#table(test$corpus == "non-writing", predictions[, 1] > 0.5)
# Model Evaluation
# creating a dataframe from known (true) test labels
test.labels <- data.frame(test$corpus)
# combining predicted and known classes
class.comparison <- data.frame(mlp.predictions.rd, test.labels)
# giving appropriate column names
names(class.comparison) <- c("predicted", "observed")
# inspecting our results table
head(class.comparison)
# get confusion matrix
cm <- confusionMatrix(as.factor(class.comparison$predicted),
reference = as.factor(class.comparison$observed))
#print(cm)
# get precision, recall, and f1 from the output list of confusionMatrix()
accuracy <- cm$overall['Accuracy']
f1 <- cm[["byClass"]]["F1"]
recall <- cm[["byClass"]]["Recall"]
precision <- cm[["byClass"]]["Precision"]
# unname and round resulting values
accuracy <- round(unname(accuracy), 2)
f1 <- round(unname(f1), 2)
recall <- round(unname(recall), 2)
precision <- round(unname(precision), 2)
# append results to dataframe
hidden.structure <- paste(unlist(hidden), collapse = ",")
hidden.size <- sum(unlist(hidden))
hidden.depth <- length(hidden)
local.df <- data.frame(num.char, hidden.structure, hidden.size,
hidden.depth, err.fct, act.fct.name, algorithm,
accuracy, precision, recall, f1)
results.df <- rbind(results.df, local.df)
})
counter <- counter + 1
print(counter)
}
end_time <- Sys.time()
proc.time <- end_time - start_time
print(proc.time)
set.seed(123)
# counter
counter <- 0
# start time
start_time <- Sys.time()
for (hidden in hidden.list) {
# Training
# choose error function
err.fct = 'ce' # options: 'sse', 'ce'
# choose activation function
act.fct = 'logistic' # options: 'logistic', 'tanh', softplus, relu
# note: softplus, relu, and tanh only work with sse as err.fct
act.fct.name = 'logistic' # make sure to give name as character string (for later storage)
# choose algorithm
algorithm = 'rprop+' # options: 'rprop+', 'rprop-', 'backprop', 'sag', 'slr'
# train classifier with neuralnet() function
try({ # if the processing failes for a certain file, there will be no output for this file,
# but the try() function allows the loop to keep running
classifier.mlp <- neuralnet(corpus == "writing" ~ .,
data = train,
hidden = hidden,
threshold = 0.1, # defaults to 0.01
rep = 1, # number of reps in which new initial values are used,
# (essentially the same as a for loop)
stepmax = 100000, # defaults to 100K
linear.output = FALSE,
algorithm = algorithm, # defaults to "rprop+",
# i.e. resilient backpropagation
# learningrate = 0.1,
err.fct = err.fct,
act.fct = act.fct,
likelihood = TRUE,
lifesign = 'minimal')
#classifier.mlp
# results matrix (each column represents one repetition)
# classifier.mlp$result.matrix
# Prediction
# Get prediction using the predict() function
mlp.predictions <- predict(classifier.mlp, test,
rep = which.min(classifier.mlp$result.matrix[1,]), # Predict response values
# based on the "best" repetition (epoche), i.e. the one with the lowest error.
all.units = FALSE)
# assign a label according to the rule that the label is "writing"
# if the prediction probability is >0.5, else assign "non-writing".
mlp.predictions.rd <- ifelse(mlp.predictions > 0.5, "writing", "non-writing")
#head(mlp.predictions.rd, 10)
#table(test$corpus == "non-writing", predictions[, 1] > 0.5)
# Model Evaluation
# creating a dataframe from known (true) test labels
test.labels <- data.frame(test$corpus)
# combining predicted and known classes
class.comparison <- data.frame(mlp.predictions.rd, test.labels)
# giving appropriate column names
names(class.comparison) <- c("predicted", "observed")
# inspecting our results table
head(class.comparison)
# get confusion matrix
cm <- confusionMatrix(as.factor(class.comparison$predicted),
reference = as.factor(class.comparison$observed))
#print(cm)
# get precision, recall, and f1 from the output list of confusionMatrix()
accuracy <- cm$overall['Accuracy']
f1 <- cm[["byClass"]]["F1"]
recall <- cm[["byClass"]]["Recall"]
precision <- cm[["byClass"]]["Precision"]
# unname and round resulting values
accuracy <- round(unname(accuracy), 2)
f1 <- round(unname(f1), 2)
recall <- round(unname(recall), 2)
precision <- round(unname(precision), 2)
# append results to dataframe
hidden.structure <- paste(unlist(hidden), collapse = ",")
hidden.size <- sum(unlist(hidden))
hidden.depth <- length(hidden)
local.df <- data.frame(num.char, hidden.structure, hidden.size,
hidden.depth, err.fct, act.fct.name, algorithm,
accuracy, precision, recall, f1)
results.df <- rbind(results.df, local.df)
})
counter <- counter + 1
print(counter)
}
end_time <- Sys.time()
proc.time <- end_time - start_time
print(proc.time)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 1000
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
estimations.subset <- estimations.df[c("corpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1], scale(estimations.subset[2:ncol(estimations.subset)]))
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
train <- estimations.scaled[datasample == 1, 1:ncol(estimations.scaled)]
# Generate test set
test <- estimations.scaled[datasample == 2, 1:ncol(estimations.scaled)]
hidden.list <- list(c(4,2))
# initialize dataframe to append results to
results.df <- data.frame(num.char = numeric(0), hidden.structure = character(0),
hidden.size = numeric(0), hidden.depth = numeric(0),
err.fct = character(0), act.fct.name = character(0),
algorithm = character(0), accuracy = numeric(0),
precision = numeric(0), recall = numeric(0),
f1 = numeric(0)
)
set.seed(123)
# counter
counter <- 0
# start time
start_time <- Sys.time()
for (hidden in hidden.list) {
# Training
# choose error function
err.fct = 'ce' # options: 'sse', 'ce'
# choose activation function
act.fct = 'logistic' # options: 'logistic', 'tanh', softplus, relu
# note: softplus, relu, and tanh only work with sse as err.fct
act.fct.name = 'logistic' # make sure to give name as character string (for later storage)
# choose algorithm
algorithm = 'rprop+' # options: 'rprop+', 'rprop-', 'backprop', 'sag', 'slr'
# train classifier with neuralnet() function
try({ # if the processing failes for a certain file, there will be no output for this file,
# but the try() function allows the loop to keep running
classifier.mlp <- neuralnet(corpus == "writing" ~ .,
data = train,
hidden = hidden,
threshold = 0.1, # defaults to 0.01
rep = 10, # number of reps in which new initial values are used,
# (essentially the same as a for loop)
stepmax = 100000, # defaults to 100K
linear.output = FALSE,
algorithm = algorithm, # defaults to "rprop+",
# i.e. resilient backpropagation
# learningrate = 0.1,
err.fct = err.fct,
act.fct = act.fct,
likelihood = TRUE,
lifesign = 'minimal')
#classifier.mlp
# results matrix (each column represents one repetition)
# classifier.mlp$result.matrix
# Prediction
# Get prediction using the predict() function
mlp.predictions <- predict(classifier.mlp, test,
rep = which.min(classifier.mlp$result.matrix[1,]), # Predict response values
# based on the "best" repetition (epoche), i.e. the one with the lowest error.
all.units = FALSE)
# assign a label according to the rule that the label is "writing"
# if the prediction probability is >0.5, else assign "non-writing".
mlp.predictions.rd <- ifelse(mlp.predictions > 0.5, "writing", "non-writing")
#head(mlp.predictions.rd, 10)
#table(test$corpus == "non-writing", predictions[, 1] > 0.5)
# Model Evaluation
# creating a dataframe from known (true) test labels
test.labels <- data.frame(test$corpus)
# combining predicted and known classes
class.comparison <- data.frame(mlp.predictions.rd, test.labels)
# giving appropriate column names
names(class.comparison) <- c("predicted", "observed")
# inspecting our results table
head(class.comparison)
# get confusion matrix
cm <- confusionMatrix(as.factor(class.comparison$predicted),
reference = as.factor(class.comparison$observed))
#print(cm)
# get precision, recall, and f1 from the output list of confusionMatrix()
accuracy <- cm$overall['Accuracy']
f1 <- cm[["byClass"]]["F1"]
recall <- cm[["byClass"]]["Recall"]
precision <- cm[["byClass"]]["Precision"]
# unname and round resulting values
accuracy <- round(unname(accuracy), 2)
f1 <- round(unname(f1), 2)
recall <- round(unname(recall), 2)
precision <- round(unname(precision), 2)
# append results to dataframe
hidden.structure <- paste(unlist(hidden), collapse = ",")
hidden.size <- sum(unlist(hidden))
hidden.depth <- length(hidden)
local.df <- data.frame(num.char, hidden.structure, hidden.size,
hidden.depth, err.fct, act.fct.name, algorithm,
accuracy, precision, recall, f1)
results.df <- rbind(results.df, local.df)
})
counter <- counter + 1
print(counter)
}
end_time <- Sys.time()
proc.time <- end_time - start_time
print(proc.time)
mlp.plot <- plot(classifier.mlp, rep = 'best', show.weights = F)
results.df
classifier.mlp
results.df
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(class)
library(gridExtra)
library(gmodels)
library(caret)
library(ggExtra)
library(ggpubr)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
num.char = 10
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
nrow(estimations.df)
estimations.subset <- estimations.df[c("corpus", "subcorpus",
"huni.chars",
"hrate.chars",
"ttr.chars",
"rm.chars"
)]
estimations.subset <- na.omit(estimations.subset)
estimations.scaled <- cbind(estimations.subset[1:2], scale(estimations.subset[3:ncol(estimations.subset)]))
nrow(estimations.scaled)
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
estimations.training <- estimations.scaled[datasample == 1, 3:ncol(estimations.scaled)]
nrow(estimations.training)
# Generate test set
estimations.test <- estimations.scaled[datasample == 2, 3:ncol(estimations.scaled)]
nrow(estimations.test)
training.labels <- estimations.scaled[datasample == 1, 1]
# Generate test labels
test.labels <- estimations.scaled[datasample == 2, 1]
knn.results <- data.frame(k = numeric(0), precision = numeric(0),
recall = numeric(0), f1 = numeric(0))
knn.results
n = 10
k = 1
predictions.knn <- knn(train = as.data.frame(estimations.training),
test = as.data.frame(estimations.test),
cl = training.labels, k = k)
predictions.knn
test.labels <- data.frame(test.labels)
# combining predicted and known classes
class.comparison <- data.frame(predictions.knn, test.labels)
# giving appropriate column names
names(class.comparison) <- c("predicted", "observed")
# inspecting our results table
head(class.comparison)
# get confusion matrix
cm <- confusionMatrix(data = class.comparison$predicted,
reference = as.factor(class.comparison$observed))
cm
install.packages("sigmoid")
?svm()
knitr::opts_chunk$set(echo = TRUE)
paste(c("~/Github/NaLaFi/results/SVM/results_SVM_",
num.char, "_", kernel, ".csv"), sep = ""), row.names = F)
paste(c("~/Github/NaLaFi/results/SVM/results_SVM_",
num.char, "_", kernel, ".csv"), collapse = "")
kernel = "radial"
paste(c("~/Github/NaLaFi/results/SVM/results_SVM_",
num.char, "_", kernel, ".csv"), collapse = "")
paste(c("~/Github/NaLaFi/results/SVM/results_SVM_",
num.char, "chars", "_", kernel, ".csv"), collapse = "")
knitr::opts_chunk$set(echo = TRUE)
mlp.plot <- plot(classifier.mlp, rep = 'best', show.weights = F)
file.choose()
knitr::opts_chunk$set(echo = TRUE)
mlp.results <- read.csv("~/home/chris/Github/NaLaFi/results/MLP/results_MLP_1000chars_logistic_ce_rprop+.csv")
mlp.results <- read.csv("~/Github/NaLaFi/results/MLP/results_MLP_1000chars_logistic_ce_rprop+.csv")
head(mlp.results)
file.choose()
knn.results <- read.csv("~/Github/NaLaFi/results/KNN/knn_results_1000.csv")
head(knn.results)
ggplot(knn.results, aes(x = k, y = f1)) +
geom_errorbar(aes(ymin=f1-sd, ymax=f1+sd), width=.1) +
geom_line() +
geom_point() +
theme_minimal()
library(ggplot2)
library(gridExtra)
ggplot(knn.results, aes(x = k, y = f1)) +
geom_errorbar(aes(ymin=f1-sd, ymax=f1+sd), width=.1) +
geom_line() +
geom_point() +
theme_minimal()
ggplot(knn.results, aes(x = k, y = f1)) +
geom_errorbar(aes(ymin=f1-sd(f1), ymax=f1+sd(f1)), width=.1) +
geom_line() +
geom_point() +
theme_minimal()
ggplot(knn.results, aes(x = k, y = f1)) +
#geom_errorbar(aes(ymin=f1-sd(f1), ymax=f1+sd(f1)), width=.1) +
geom_line() +
geom_point() +
theme_minimal()
ggplot(knn.results, aes(x = k, y = f1)) +
#geom_errorbar(aes(ymin=f1-sd(f1), ymax=f1+sd(f1)), width=.1) +
geom_line() +
geom_point()
knitr::opts_chunk$set(echo = TRUE)
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
head(estimations.df)
unique(estimations.df$subcorpus)
selected <- c("random")
estimations.df <- estimations.df[!(estimations.df$subcorpus %in% selected), ]
View(estimations.df)
