---
title: "Classification with linear SVM"
author: "Chris Bentz"
date: "14/10/2023"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Description
Support vector machine (SVM) analyses of the feature vectors per character string (loaded from NaLaFi/results/features.csv). The results are stored in NaLaFi/results/SVM. Note that the number of characters has to be chosen manually (via num.char = ``''). Note that the kernel (linear, polynomial, radial, sigmoid) has to be chosen manually below.

# Load Packages
If the libraries are not installed yet, you need to install them using, for example, the command: install.packages("ggplot2"). For the Hrate package this is different, since it comes from github. The devtools library needs to be installed, and then the install_github() function is used.
```{r, message = F}
library(dplyr)
library(e1071)
library(class)
library(gmodels)
library(ggpubr)
library(caret)
```

# Load Data
Load data table with values per text file. 
```{r}
# load estimations from stringBase corpus
estimations.df <- read.csv("~/Github/NaLaFi/results/features.csv")
#head(features.csv)
```

Exclude subcorpora (if needed).
```{r}
#selected <- c("shuffled", "random")
#estimations.df <- estimations.df[!(estimations.df$subcorpus %in% selected), ]
```

Split into separate files by length of chunks in characters.
```{r}
# choose number of characters
num.char = 1000
# subset data frame
estimations.df <- estimations.df[estimations.df$num.char == num.char, ]
```

Select relevant columns of the data frame, i.e. the measures to be included in classification and the ''corpus'' or ''subcorpus'' column.
```{r}
estimations.subset <- estimations.df[c("corpus",
                                       "huni.chars", 
                                       "hrate.chars", 
                                       "ttr.chars", 
                                       "rm.chars"
                                       )]
```

Remove NAs (whole row)
```{r}
estimations.subset <- na.omit(estimations.subset)
```

# Center and scale the data
```{r}
estimations.scaled <- cbind(estimations.subset[1], scale(estimations.subset[2:ncol(estimations.subset)]))
```

# Create Training and Test Sets
```{r}
# Generating seed
set.seed(1234)
# Randomly generating our training and test samples with a respective ratio of 2/3 and 1/3
datasample <- sample(2, nrow(estimations.scaled), replace = TRUE, prob = c(0.67, 0.33))
# Generate training set
train <- estimations.scaled[datasample == 1, 1:ncol(estimations.scaled)]
# Generate test set 
test <- estimations.scaled[datasample == 2, 1:ncol(estimations.scaled)]
```

# Building SVM
The following code to build a linear SVM is adopted from https://rpubs.com/cliex159/865583 (last accessed 17.01.2023).
```{r}
# choose kernel
kernel = "sigmoid" # "linear", "polynomial", "radial", "sigmoid"

# svm classification
svm.model <- svm(as.factor(corpus) ~ ., 
                data = train, 
                type = "C-classification", 
                kernel = kernel, 
                scale = FALSE)
# list components of model
names(svm.model)
# check the levels
svm.model$levels
```

# Prediction
Make predictions using the svm model.
```{r}
svm.prediction <- predict(svm.model, test)
```

# Model evaluation
```{r}
# creating a dataframe from known (true) test labels
test.labels <- data.frame(test$corpus)
# combining predicted and known classes
class.comparison <- data.frame(svm.prediction, test.labels)
# giving appropriate column names
names(class.comparison) <- c("predicted", "observed")
# inspecting our results table
head(class.comparison)
# get confusion matrix
cm <- confusionMatrix(as.factor(class.comparison$predicted), 
                      reference = as.factor(class.comparison$observed))
print(cm)
# get precision, recall, and f1 from the output list of confusionMatrix()
accuracy <- cm$overall['Accuracy']
f1 <- cm[["byClass"]]["F1"]
recall <- cm[["byClass"]]["Recall"]
precision <- cm[["byClass"]]["Precision"]
  
# prepare data frame with results
svm.results <- data.frame(accuracy, precision, recall, f1, row.names = NULL)
svm.results.rounded <- round(svm.results, 2)
print(svm.results.rounded)
```

Write to file.
```{r}
write.csv(svm.results.rounded, file = paste(c("~/Github/NaLaFi/results/SVM/results_SVM_", 
                                              num.char, "chars", "_", kernel, ".csv"), collapse = ""), 
          row.names = F)
```

# Visualization
Build scatter plot of training dataset.

## Unigram entropy and TTR
```{r}
# downsample the training dataset (otherwise there is a lot of overplotting)
train.ds <- sample_n(train, 100)

huni.ttr.plot <- ggplot(data = train.ds, aes(x = huni.chars, y = ttr.chars, color = corpus)) + 
    geom_point() +
    labs(x = "Unigram entropy for 100 characters", y = "TTR for 100 characters") 
#huni.ttr.plot

# svm classification (with just entropy rate and repetition rate)
svm.model <- svm(as.factor(corpus) ~ huni.chars + ttr.chars, 
                data = train.ds, 
                type = "C-classification", 
                kernel = "linear", 
                scale = FALSE)

w <- t(svm.model$coefs) %*% svm.model$SV
# calculate slope and intercept of decision boundary from weight vector and svm model
slope_1 <- -w[1]/w[2]
intercept_1 <- svm.model$rho/w[2]

#add decision boundary 
plot.decision <- huni.ttr.plot + geom_abline(slope = slope_1, intercept = intercept_1) 

#add margin boundaries
plot.margins <- plot.decision + 
 geom_abline(slope = slope_1, intercept = intercept_1 - 1/w[2], linetype = "dashed")+
 geom_abline(slope = slope_1, intercept = intercept_1 + 1/w[2], linetype = "dashed")

# add suport vectors
layered.huni.ttr.plot <- 
    plot.margins + geom_point(data = train.ds[svm.model$index, ], aes(x = huni.chars, y = ttr.chars), color = "purple", size = 4, alpha = 0.3)

# show plot
#layered.huni.ttr.plot
```

## Entropy rate and repetition rate
```{r}
# downsample the training dataset (otherwise there is a lot of overplotting)
train.ds <- sample_n(train, 100)

# create ggplot
hrate.rm.plot <- ggplot(data = train.ds, aes(x = hrate.chars, y = rm.chars, color = corpus)) + 
    geom_point() +
    labs(x = "Entropy rate for 100 characters", y = "Repetition rate for 100 characters") +
    theme(legend.position = "none")
#hrate.rm.plot

# svm classification (with just entropy rate and repetition rate)
svm.model <- svm(as.factor(corpus) ~ hrate.chars + rm.chars, 
                data = train.ds, 
                type = "C-classification", 
                kernel = "linear", 
                scale = FALSE)

w <- t(svm.model$coefs) %*% svm.model$SV
# calculate slope and intercept of decision boundary from weight vector and svm model
slope_1 <- -w[1]/w[2]
intercept_1 <- svm.model$rho/w[2]

#add decision boundary 
plot.decision <- hrate.rm.plot + geom_abline(slope = slope_1, intercept = intercept_1) 

#add margin boundaries
plot.margins <- plot.decision + 
 geom_abline(slope = slope_1, intercept = intercept_1 - 1/w[2], linetype = "dashed")+
 geom_abline(slope = slope_1, intercept = intercept_1 + 1/w[2], linetype = "dashed")

# add suport vectors
layered.hrate.rm.plot <- 
    plot.margins + geom_point(data = train.ds[svm.model$index, ], aes(x = hrate.chars, y = rm.chars), color = "purple", size = 4, alpha = 0.3)

# show plot
#layered.hrate.rm.plot
```

## Combined Plots
```{r, fig.width = 10, fig.height = 4}
plots.combined <- ggarrange(layered.huni.ttr.plot, layered.hrate.rm.plot,
                    labels = c("a)", "b)"),
                    ncol = 2, widths = c(1.3, 1))
plots.combined
```

## Safe complete figure to file
```{r}
ggsave("~/Github/NaLaFi/figures/plots_SVM.pdf", plots.combined, width = 10, 
       height = 4, dpi = 300, scale = 1, device = cairo_pdf)
```
